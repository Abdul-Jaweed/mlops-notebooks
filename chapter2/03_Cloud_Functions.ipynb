{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## GCP Cloud Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "### Intro\n",
    "\n",
    " - Cloud Functions (CFs) are a solution from GCP for serverless deployments.\n",
    " - Very little boilerplate beyond what we will write for simple offline model inference.\n",
    " - In any such deployment, we need to be concerned about: \n",
    "   - where the model is stored (recall pickle and mlflow), and\n",
    "   - what python packages are available.\n",
    "\n",
    "### Empty Deployment\n",
    "\n",
    " - We will set up triggers that will trigger our serving function (in particular, a HTTP request).\n",
    " - We will specify the requirements needed for our python function to work\n",
    " - The function we deploy here, similar to lecture 1, produces weather forecasts given a location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Setting up using UI\n",
    "\n",
    " - Sign up with GCP if you haven't already (typically you get a 300$ credit)\n",
    "\n",
    " - Get to the console and find the Cloud Function page.\n",
    "\n",
    " <img src=\"images/cf_landing.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    "\n",
    " - Go through the UI for creating a function.\n",
    "\n",
    "  <img src=\"images/cf_create_function.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    "\n",
    "\n",
    " - We will choose the HTTP trigger and unauthenticated access option.\n",
    "\n",
    "   <img src=\"images/cf_http_trigger.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    "\n",
    " - We may have to enable Cloud Build API if AWS prompts it (when we are setting up for the first time).\n",
    "\n",
    "    <img src=\"images/cf_cloudbuild.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    "\n",
    " - Finally, we choose the Python environment. You can see two default example files (main.py and requirements.txt). We will be modifying these two.\n",
    "\n",
    "     <img src=\"images/cf_python_env.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Python Files and Requirements\n",
    "\n",
    " - In the following description, we will assume that we want to deploy the weather service example (the random food image example as well as model deployment examples would follow a similar process).\n",
    " - If we want to deploy the weather service app, then we will specify the following requirements:\n",
    "\n",
    " ```python\n",
    "flask\n",
    "geopy\n",
    "requests\n",
    " ```\n",
    "\n",
    " - Our main file would be the following:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(request):\n",
    "    from flask import jsonify\n",
    "    from geopy.geocoders import Nominatim\n",
    "    import requests\n",
    "\n",
    "    data = {\"success\": False}\n",
    "    #https://pypi.org/project/geopy/\n",
    "    geolocator = Nominatim(user_agent=\"cloud_function_weather_app\")\n",
    "    params = request.get_json()\n",
    "    if \"msg\" in params:\n",
    "        location = geolocator.geocode(str(params['msg']))\n",
    "        # https://www.weather.gov/documentation/services-web-api\n",
    "        # Example query: https://api.weather.gov/points/39.7456,-97.0892\n",
    "        result1 = requests.get(f\"https://api.weather.gov/points/{location.latitude},{location.longitude}\")\n",
    "        # Example query: https://api.weather.gov/gridpoints/TOP/31,80\n",
    "        result2 = requests.get(f\"{result1.json()['properties']['forecast']}\")\n",
    "        data[\"response\"] = result2.json()\n",
    "        data[\"success\"] = True\n",
    "    return jsonify(data)"
   ]
  },
  {
   "source": [
    " - Once the function is deployed, we can test the function (click actions on the far right in the dashboard)\n",
    "\n",
    "  <img src=\"images/cf_dashboard.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    " - We can pass the JSON string `{\"msg\":\"Chicago\"}` and see that we indeed get the JSON output for the weather of Chicago.\n",
    "\n",
    " <img src=\"images/cf_test.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    " - We can also access the function from the web endpoint (e.g., something like `https://us-central1-project-name-replace-here.cloudfunctions.net/function-1`). Note that unlike previous times, the request to this endpoint is a JSON payload.\n",
    "\n",
    " - Below is the screen-shot of querying the weather of Chicago using the `Postman` tool. The way to use it is as follows:\n",
    "\n",
    "    1. Insert the URL of the API\n",
    "    2. Se the method type to POST\n",
    "    3. Navigate to body and choose `raw` and then choose `JSON` from the dropdown menu.\n",
    "    4. Now add the relevant parameters as a JSON string.\n",
    "\n",
    " <img src=\"images/cf_postman.png\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\"/>\n",
    "\n",
    " - Finally, here is a query you can use from a Jupyter notebook.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "result = requests.post(\n",
    "        \"https://us-central1-[project-name-replace-here].cloudfunctions.net/function-1\"\n",
    "                    ,json = { 'msg': 'Chicago' })\n",
    "print(result.json())\n",
    "#should match with https://forecast.weather.gov/MapClick.php?textField1=41.98&textField2=-87.9"
   ]
  },
  {
   "source": [
    "### Saving Model on the Cloud\n",
    "\n",
    " - For our original task of deploying a trained ML model, we need a way to read it from somewhere when the function is triggered.\n",
    "\n",
    " - One way is to dump the model and related metadata onto Google Cloud Storage (GCS). This way everything is within the same cloud infrastructure and permissions and access controls becomes a bit easier.\n",
    "\n",
    " - GCS is similar to the S3 (simple storage service) by AWS (which we will look at shortly).\n",
    "\n",
    " - We will use the command line to dump our model onto the cloud."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### GCP access via the commandline\n",
    "\n",
    " - First we need to install the Google Cloud [SDK](https://cloud.google.com/sdk/install) from [https://cloud.google.com/sdk/docs/downloads-interactive](https://cloud.google.com/sdk/docs/downloads-interactive)\n",
    "\n",
    " ```bash\n",
    "curl https://sdk.cloud.google.com | bash\n",
    "gcloud init\n",
    " ```\n",
    "\n",
    " - There are two types of accounts you can work with: a user account or a service account (see [https://cloud.google.com/sdk/docs/authorizing?authuser=2](https://cloud.google.com/sdk/docs/authorizing?authuser=2)). \n",
    "\n",
    " - We will create a service account (so that it can be used by an application programmatically anywhere) and store the encrypted credentials on disk for programmatic access through python. To do so, we run the following commands.\n",
    "\n",
    " - We [create the service account]( https://cloud.google.com/iam/docs/creating-managing-service-accounts#iam-service-accounts-list-gcloud) and check that it is active by using this command: `gcloud iam service-accounts list`.\n",
    "\n",
    "```bash\n",
    "gcloud iam service-accounts create idsservice \\\n",
    "    --description=\"IDS service account\" \\\n",
    "    --display-name=\"idsservice-displayed\"\n",
    "```\n",
    "\n",
    "  - We then [assign a new role](https://cloud.google.com/iam/docs/granting-changing-revoking-access) for this service account in the project. The account can be disabled using the command `gcloud iam service-accounts disable idsservice@authentic-realm-276822.iam.gserviceaccount.com` (change idsservice and authentic-realm-276822 to your specific names).\n",
    "\n",
    "```bash\n",
    "gcloud projects add-iam-policy-binding authentic-realm-276822     --member=serviceAccount:idsservice@authentic-realm-276822.iam.gserviceaccount.com --role=roles/owner\n",
    "```\n",
    "\n",
    "\n",
    "  - Finally, we can [download the credentials](https://cloud.google.com/iam/docs/creating-managing-service-account-keys)\n",
    "\n",
    "```bash\n",
    "gcloud iam service-accounts keys create ~/idsservice.json \\\n",
    "  --iam-account idsservice@authentic-realm-276822.iam.gserviceaccount.com\n",
    "```\n",
    "\n",
    "  - Once the credentials are downloaded, they can be programmatically accessed using python running on that machine. We just have to explore the location of the file:\n",
    "\n",
    "```bash\n",
    " export GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/idsservice.json\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " - Next we will install a python module to access GCS, so that we can write our model to the cloud:\n",
    "\n",
    " ```bash\n",
    "pip install google-cloud-storage\n",
    " ```\n",
    "\n",
    " - The following code creates a bucket called `theja_model_store` \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"theja_model_store\"\n",
    "storage_client = storage.Client()\n",
    "storage_client.create_bucket(bucket_name)\n",
    "for bucket in storage_client.list_buckets():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "source": [
    "We can dump the model (and optionally metadata) we used previously here using the following snippet. For example, lets pick the recommendation model example. In this case, we would upload two files, the model file and the movies data file, into the `theja_model_store` bucket. One can upload any files in general, but for our setting, these two (the trained model and the movie metadata file) are of interest. If we need access to the dataset as well to make new predictions, then one may need to upload the dataset as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"theja_model_store\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(\"serverless/recommendation_model_related/model\")\n",
    "blob.upload_from_filename(\"recommendation_model_pytorch.pkl\")\n",
    "blob = bucket.blob(\"serverless/recommendation_model_related/movies-metadata\")\n",
    "blob.upload_from_filename(\"movies.dat\")"
   ]
  },
  {
   "source": [
    " ![storage](images/cf_storage.png)\n",
    "\n",
    "After running the above, the surprise package based recommendation model and the helper data file will be available at `gs://theja_model_store/serverless/recommendation_model_related/model` and `gs://theja_model_store/serverless/recommendation_model_related/movie-metadata` as seen below.\n",
    "\n",
    "\n",
    " ![storage_uri](images/cf_storage_uri.png)\n",
    "\n",
    " We can either use the URIs above or use a programmatic way with the storage class. For example, here is the way to download the file `model`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"theja_model_store\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(\"serverless/recommendation_model_related/model\")\n",
    "blob.download_to_filename(\"recommendation_model_pytorch.from_gcp\")"
   ]
  },
  {
   "source": [
    "If we had downloaded the movies metadata file, we could diff it in Jupyter notebook itself using the expression `!diff movies.dat movies.dat.from_gcp`.\n",
    "\n",
    "\n",
    "We will use this programmatic way of reading external data/model in the cloud function next.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}