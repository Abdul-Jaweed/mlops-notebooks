{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## GCP Cloud Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "### Intro\n",
    "\n",
    " - Cloud Functions (CFs) are a solution from GCP for serverless deployments.\n",
    " - Very little boilerplate beyond what we will write for simple offline model inference.\n",
    " - In any such deployment, we need to be concerned about: \n",
    "   - where the model is stored (recall pickle and mlflow), and\n",
    "   - what python packages are available.\n",
    "\n",
    "### Empty Deployment\n",
    "\n",
    " - We will set up triggers that will trigger our serving function (in particular, a HTTP request).\n",
    " - We will specify the requirements needed for our python function to work\n",
    " - The function we deploy here, similar to lecture 1, produces weather forecasts given a location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Setting up using UI\n",
    "\n",
    " - Sign up with GCP if you haven't already (typically you get a 300$ credit)\n",
    "\n",
    " - Get to the console and find the Cloud Function page.\n",
    "\n",
    "\n",
    " ![landing](images/cf_landing.png)\n",
    "\n",
    "\n",
    " - Go through the UI for creating a function.\n",
    "\n",
    " ![create](images/cf_create_function.png)\n",
    "\n",
    "\n",
    " - We will choose the HTTP trigger and unauthenticated access option.\n",
    "\n",
    " ![trigger](images/cf_http_trigger.png)\n",
    "\n",
    " - We may have to enable Cloud Build API\n",
    "\n",
    " ![cloudbild](images/cf_cloudbuild.png)\n",
    "\n",
    " - Finally, we choose the Python environment. You can see two default example files (main.py and requirements.txt). We will be modifying these two.\n",
    "\n",
    " ![python](images/cf_python_env.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Python Files and Requirements\n",
    "\n",
    " - We will specify the following requirements:\n",
    "\n",
    " ```python\n",
    "flask\n",
    "geopy\n",
    "requests\n",
    "\n",
    " ```\n",
    "\n",
    " - Our main file is the following:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(request):\n",
    "    from flask import jsonify\n",
    "    from geopy.geocoders import Nominatim\n",
    "    import requests\n",
    "\n",
    "    data = {\"success\": False}\n",
    "    #https://pypi.org/project/geopy/\n",
    "    geolocator = Nominatim(user_agent=\"cloud_function_weather_app\")\n",
    "    params = request.get_json()\n",
    "    if \"msg\" in params:\n",
    "        location = geolocator.geocode(str(params['msg']))\n",
    "        # https://www.weather.gov/documentation/services-web-api\n",
    "        # Example query: https://api.weather.gov/points/39.7456,-97.0892\n",
    "        result1 = requests.get(f\"https://api.weather.gov/points/{location.latitude},{location.longitude}\")\n",
    "        # Example query: https://api.weather.gov/gridpoints/TOP/31,80\n",
    "        result2 = requests.get(f\"{result1.json()['properties']['forecast']}\")\n",
    "        data[\"response\"] = result2.json()\n",
    "        data[\"success\"] = True\n",
    "    return jsonify(data)"
   ]
  },
  {
   "source": [
    " - Once the function is deployed, we can test the function (click actions on the far right in the dashboard)\n",
    "\n",
    "  ![dashboard](images/cf_dashboard.png)\n",
    "\n",
    " - We can pass the JSON string `{\"msg\":\"Chicago\"}` and see that we indeed get the JSON output for the weather of Chicago.\n",
    "\n",
    " ![test](images/cf_test.png)\n",
    "\n",
    " - We can also access the function from the web endpoint (e.g., something like `https://us-central1-project-name-replace-here.cloudfunctions.net/function-1`). Note that unlike previous times, the request to this endpoint is a JSON payload.\n",
    "\n",
    " - Below is the screen-shot of querying the weather of Chicago using the `Postman` tool. The way to use it is as follows:\n",
    "\n",
    "    1. Insert the URL of the API\n",
    "    2. Se the method type to POST\n",
    "    3. Navigate to body and choose `raw` and then choose `JSON` from the dropdown menu.\n",
    "    4. Now add the relevant parameters as a JSON string.\n",
    "\n",
    "\n",
    " ![test](images/cf_postman.png?classes=border)\n",
    "\n",
    " - Finally, here is a query you can use from a Jupyter notebook.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "result = requests.post(\n",
    "        \"https://us-central1-project-name-replace-here.cloudfunctions.net/function-1\"\n",
    "                    ,json = { 'msg': 'Chicago' })\n",
    "print(result.json())\n",
    "#should match with https://forecast.weather.gov/MapClick.php?textField1=41.98&textField2=-87.9"
   ]
  },
  {
   "source": [
    "### Saving Model on the Cloud\n",
    "\n",
    " - For our original task of deploying a trained ML model, we need a way to read it from somewhere when the function is triggered.\n",
    "\n",
    " - One way is to dump the model onto Google Cloud Storage (GCS)\n",
    "\n",
    " - GCS is similar to the S3 (simple storage service) by AWS.\n",
    "\n",
    " - We will use the command line to dump our model onto the cloud."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### GCP access via the commandline\n",
    "\n",
    " - First we need to install the Google Cloud [SDK](https://cloud.google.com/sdk/install) from [https://cloud.google.com/sdk/docs/downloads-interactive](https://cloud.google.com/sdk/docs/downloads-interactive)\n",
    "\n",
    " ```bash\n",
    "curl https://sdk.cloud.google.com | bash\n",
    "gcloud init\n",
    " ```\n",
    "\n",
    " - There are two types of accounts you can work with: a user account or a service account (see [https://cloud.google.com/sdk/docs/authorizing?authuser=2](https://cloud.google.com/sdk/docs/authorizing?authuser=2)). \n",
    "\n",
    " - Among others, [this page] gives a brief idea of why such an account is needed. In particular, we will create a service account (so that it can be used by an application programmatically anywhere) and store the encrypted credentials on disk for programmatic access through python. To do so, we run the following commands:\n",
    "\n",
    "    1. We [create the service account]( https://cloud.google.com/iam/docs/creating-managing-service-accounts#iam-service-accounts-list-gcloud) and check that it is active by using this command: `gcloud iam service-accounts list`.\n",
    "\n",
    "```bash\n",
    "gcloud iam service-accounts create idsservice \\\n",
    "    --description=\"IDS service account\" \\\n",
    "    --display-name=\"idsservice-displayed\"\n",
    "```\n",
    "\n",
    "    2. We then [assign a new role](https://cloud.google.com/iam/docs/granting-changing-revoking-access) for this service account in the project. The account can be disabled using the command `gcloud iam service-accounts disable idsservice@authentic-realm-276822.iam.gserviceaccount.com` (change idsservice and authentic-realm-276822 to your specific names).\n",
    "\n",
    "```bash\n",
    "gcloud projects add-iam-policy-binding authentic-realm-276822     --member=serviceAccount:idsservice@authentic-realm-276822.iam.gserviceaccount.com --role=roles/owner\n",
    "```\n",
    "\n",
    "\n",
    "    3. Finally, we can [download the credentials](https://cloud.google.com/iam/docs/creating-managing-service-account-keys)\n",
    "\n",
    "```bash\n",
    "gcloud iam service-accounts keys create ~/idsservice.json \\\n",
    "  --iam-account idsservice@authentic-realm-276822.iam.gserviceaccount.com\n",
    "```\n",
    "\n",
    "    4. Once the credentials are downloaded, they can be programmatically accessed using python running on that machine. We just have to explore the location of the file:\n",
    "\n",
    "```bash\n",
    " export GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/idsservice.json\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " - Next we will install a python module to access GCS, so that we can write our model to the cloud:\n",
    "\n",
    " ```bash\n",
    "pip install google-cloud-storage\n",
    " ```\n",
    "\n",
    " - The following code creates a bucket called `theja_model_store` \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"theja_model_store\"\n",
    "storage_client = storage.Client()\n",
    "storage_client.create_bucket(bucket_name)\n",
    "for bucket in storage_client.list_buckets():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "source": [
    " - We can dump the model we used previous here using the following snippet. Here, v1a and v1b are just the new names of our `surprise_model` and `movies.dat` files. We could have given the original names (surprise_model and movies.dat) instead of v1a and v1b. These are the two files we are uploading into the `theja_model_store` bucket. One can upload any files in general, but for our setting, these two (the trained model and the movie metadata file) are of interest. If we need access to the dataset as well to make new predictions, then one may need to upload the dataset as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"theja_model_store\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(\"serverless/surprise_model/v1a\")\n",
    "blob.upload_from_filename(\"surprise_model\")\n",
    "blob = bucket.blob(\"serverless/surprise_model/v1b\")\n",
    "blob.upload_from_filename(\"movies.dat\")"
   ]
  },
  {
   "source": [
    " ![storage](images/cf_storage.png)\n",
    "\n",
    "After running the above, the surprise package based recommendation model and the helper data file will be available at `gs://theja_model_store/serverless/surprise_model/v1a` and `gs://theja_model_store/serverless/surprise_model/v1b` as seen below.\n",
    "\n",
    "\n",
    " ![storage_uri](images/cf_storage_uri.png)\n",
    "\n",
    " We can either use the URIs above or use a programmatic way with the storage class. For example, here is the way to download the file `v1b`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = \"theja_model_store\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(\"serverless/surprise_model/v1b\")\n",
    "blob.download_to_filename(\"movies.dat.from_gcp\")"
   ]
  },
  {
   "source": [
    "We can diff it in Jupyter notebook itself using the expression `!diff movies.dat movies.dat.from_gcp`.\n",
    "\n",
    "\n",
    "We will use this programmatic way of reading external data/model in the cloud function next.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}