{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Serverless Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Storing the Model on S3\n",
    "\n",
    " - To set up S3 for model serving, we have to perform a number of steps.\n",
    " - We start with the s3 page.\n",
    "\n",
    "![s3](images/s3.png)\n",
    " - Create a bucket with an informative name.\n",
    "\n",
    "![s3_create1](images/s3_create1.png)\n",
    "\n",
    " - We don't have to touch any of these for now.\n",
    "\n",
    "![s3_create2](images/s3_create2.png)\n",
    "\n",
    "![s3_create3](images/s3_create3.png)\n",
    "\n",
    " - Here the summary to review.\n",
    "\n",
    "![s3_create4](images/s3_create4.png)\n",
    "\n",
    " - And we can see the bucket in the list of buckets.\n",
    "\n",
    "![s3_create5](images/s3_create5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Zip of Local Environment\n",
    "\n",
    " - We need a zip of local environment that includes all dependent libraries.\n",
    " - This is because there is no way to specify requirements.txt like in Cloud Functions.\n",
    " - This zip file can be uploaded to the bucket we created above.\n",
    " - We start with creating a directory of all dependencies.\n",
    "\n",
    "```bash\n",
    "mkdir lambda_model\n",
    "cd lambda_model\n",
    "# pip install pickle5 -t . #if we had a specific requirement, we would execute this line\n",
    "```\n",
    "\n",
    " - First we will read the model and the `movie.dat` files (presumably in the parent directory) to create two new dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the recommendations and movie info as dictionaries.\n",
    "\n",
    "from surprise import Dataset\n",
    "import pandas as pd\n",
    "import pickle5\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load(file_name):\n",
    "    dump_obj = pickle5.load(open(file_name, 'rb'))\n",
    "    return dump_obj['predictions'], dump_obj['algo']\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = {}\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        if uid not in top_n:\n",
    "            top_n[uid] = []\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def defaultdict(default_type):\n",
    "    class DefaultDict(dict):\n",
    "        def __getitem__(self, key):\n",
    "            if key not in self:\n",
    "                dict.__setitem__(self, key, default_type())\n",
    "            return dict.__getitem__(self, key)\n",
    "    return DefaultDict()\n",
    "\n",
    "df = pd.read_csv('../movies.dat',sep=\"::\",header=None,engine='python')\n",
    "df.columns = ['iid','name','genre']\n",
    "df.set_index('iid',inplace=True)\n",
    "predictions, algo = load('../surprise_model')\n",
    "top_n = get_top_n(predictions, n=5)\n",
    "df = df.drop(['genre'],axis=1)\n",
    "movie_dict = df.T.to_dict()\n",
    "pickle.dump(movie_dict,open('movie_dict.pkl','wb'))\n",
    "pickle.dump(top_n,open('top_n.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Next we will create the following file called `lambda_function.py` in this directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# top_n = {'196':[(1,3),(2,4)]}\n",
    "# movie_dict = {1:{'name':'a'},2:{'name':'b'}}\n",
    "\n",
    "def lambda_handler(event,context):\n",
    "    data = {\"success\": False}\n",
    "\n",
    "\n",
    "    with open(\"top_n.json\", \"r\") as read_file:\n",
    "        top_n = json.load(read_file)\n",
    "    with open(\"movie_dict.json\", \"r\") as read_file:\n",
    "        movie_dict = json.load(read_file)\n",
    "\n",
    "\n",
    "    print(event) #debug\n",
    "    if \"body\" in event:\n",
    "        event = event[\"body\"]\n",
    "        if event is not None:\n",
    "            event = json.loads(event)\n",
    "        else:\n",
    "            event = {}\n",
    "\n",
    "    if \"uid\" in event: \n",
    "        data[\"response\"] = str([movie_dict.get(iid,{'name':None})['name'] for (iid, _) in top_n[event.get(\"uid\")]])\n",
    "        data[\"success\"] = True\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'headers':{'Content-Type':'application/json'},\n",
    "        'body': json.dumps(data)\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Here instead of a request object in GCP, a pair `(event,context)` are taken as input. The `event` object will have the query values. See [this](https://docs.aws.amazon.com/lambda/latest/dg/python-handler.html) for more details.\n",
    "\n",
    " - Next we zip the model and its dependencies and upload to S3.\n",
    "\n",
    "```bash\n",
    "zip -r recommend.zip .\n",
    "aws s3 cp recommend.zip s3://theja-model-store/recommend.zip \n",
    "aws s3 ls s3://theja-model-store/\n",
    "``` \n",
    "\n",
    " - Add the API Gateway as before and see the predictions in action!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
