{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Recommendation Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will look at two models for recommending movies to existing users. \n",
    "\n",
    " - Matrix factorization based on the surprise package.\n",
    " - Matrix factorization based on Pytorch.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommendation (Pytorch) Training\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "Please install the package using the command `conda install -c conda-forge scikit-surprise` in the ight environment.\n",
    "\n",
    "\n",
    "```python\n",
    "# https://github.com/NicolasHug/Surprise\n",
    "from surprise import SVD, Dataset\n",
    "from surprise.accuracy import rmse\n",
    "from surprise.dump import dump\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss, MeanSquaredError\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class Loader():\n",
    "    current = 0\n",
    "\n",
    "    def __init__(self, x, y, batchsize=1024, do_shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batchsize = batchsize\n",
    "        self.batches = range(0, len(self.y), batchsize)\n",
    "        if do_shuffle:\n",
    "            # Every epoch re-shuffle the dataset\n",
    "            self.x, self.y = shuffle(self.x, self.y)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Reset & return a new iterator\n",
    "        self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches\n",
    "        return int(len(self.x) / self.batchsize)\n",
    "\n",
    "    def __next__(self):\n",
    "        n = self.batchsize\n",
    "        if self.current + n >= len(self.y):\n",
    "            raise StopIteration\n",
    "        i = self.current\n",
    "        xs = torch.from_numpy(self.x[i:i + n])\n",
    "        ys = torch.from_numpy(self.y[i:i + n])\n",
    "        self.current += n\n",
    "        return (xs, ys)\n",
    "\n",
    "\n",
    "\n",
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class MF(nn.Module):\n",
    "    itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0):\n",
    "        super(MF, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def __call__(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        \n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        return total\n",
    "\n",
    "def log_training_loss(engine, log_interval=400):\n",
    "    epoch = engine.state.epoch\n",
    "    itr = engine.state.iteration\n",
    "    fmt = \"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
    "    msg = fmt.format(epoch, itr, len(train_loader), engine.state.output)\n",
    "    model.itr = itr\n",
    "    if itr % log_interval == 0:\n",
    "        print(msg)\n",
    "\n",
    "def log_validation_results(engine):\n",
    "    evaluat.run(test_loader)\n",
    "    metrics = evaluat.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    print(\"Epoch[{}] Validation MSE: {:.2f} \"\n",
    "          .format(engine.state.epoch, avg_accuracy))    \n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "#Data\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "uir = np.array([x for x in trainset.all_ratings()])\n",
    "train_x = test_x = uir[:,:2].astype(np.int64)\n",
    "train_y = test_y = uir[:,2].astype(np.float32)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "#Parameters\n",
    "lr = 1e-2\n",
    "k = 10 #latent dimension\n",
    "c_bias = 1e-6\n",
    "c_vector = 1e-6\n",
    "batchsize = 1024\n",
    "\n",
    "model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "trainer = create_supervised_trainer(model, optimizer, model.loss)\n",
    "metrics = {'accuracy': MeanSquaredError()}\n",
    "\n",
    "evaluat = create_supervised_evaluator(model, metrics=metrics)\n",
    "train_loader = Loader(train_x, train_y, batchsize=batchsize)\n",
    "test_loader = Loader(test_x, test_y, batchsize=batchsize)\n",
    "trainer.add_event_handler(event_name=Events.ITERATION_COMPLETED, handler=log_training_loss)\n",
    "trainer.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=log_validation_results)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    <ignite.engine.events.RemovableEventHandle at 0x7f011e8a67c0>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "model\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    MF(\n",
    "      (user): Embedding(943, 10)\n",
    "      (item): Embedding(1682, 10)\n",
    "      (bias_user): Embedding(943, 1)\n",
    "      (bias_item): Embedding(1682, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "trainer.run(train_loader, max_epochs=50)\n",
    "```\n",
    "\n",
    "    Epoch[1] Validation MSE: 17.82 \n",
    "    Epoch[2] Validation MSE: 16.02 \n",
    "    Epoch[3] Validation MSE: 14.43 \n",
    "    Epoch[4] Validation MSE: 13.04 \n",
    "    Epoch[5] Iteration[400/97] Loss: 12.52\n",
    "    Epoch[5] Validation MSE: 11.79 \n",
    "    Epoch[6] Validation MSE: 10.70 \n",
    "    Epoch[7] Validation MSE: 9.71 \n",
    "    Epoch[8] Validation MSE: 8.85 \n",
    "    Epoch[9] Iteration[800/97] Loss: 8.95\n",
    "    Epoch[9] Validation MSE: 8.08 \n",
    "    Epoch[10] Validation MSE: 7.40 \n",
    "    Epoch[11] Validation MSE: 6.80 \n",
    "    Epoch[12] Validation MSE: 6.27 \n",
    "    Epoch[13] Iteration[1200/97] Loss: 6.50\n",
    "    Epoch[13] Validation MSE: 5.79 \n",
    "    Epoch[14] Validation MSE: 5.36 \n",
    "    Epoch[15] Validation MSE: 4.98 \n",
    "    Epoch[16] Validation MSE: 4.63 \n",
    "    Epoch[17] Iteration[1600/97] Loss: 4.80\n",
    "    Epoch[17] Validation MSE: 4.32 \n",
    "    Epoch[18] Validation MSE: 4.04 \n",
    "    Epoch[19] Validation MSE: 3.79 \n",
    "    Epoch[20] Validation MSE: 3.56 \n",
    "    Epoch[21] Iteration[2000/97] Loss: 3.35\n",
    "    Epoch[21] Validation MSE: 3.35 \n",
    "    Epoch[22] Validation MSE: 3.15 \n",
    "    Epoch[23] Validation MSE: 2.97 \n",
    "    Epoch[24] Validation MSE: 2.81 \n",
    "    Epoch[25] Iteration[2400/97] Loss: 2.73\n",
    "    Epoch[25] Validation MSE: 2.66 \n",
    "    Epoch[26] Validation MSE: 2.53 \n",
    "    Epoch[27] Validation MSE: 2.40 \n",
    "    Epoch[28] Validation MSE: 2.28 \n",
    "    Epoch[29] Iteration[2800/97] Loss: 2.31\n",
    "    Epoch[29] Validation MSE: 2.17 \n",
    "    Epoch[30] Validation MSE: 2.07 \n",
    "    Epoch[31] Validation MSE: 1.97 \n",
    "    Epoch[32] Validation MSE: 1.89 \n",
    "    Epoch[33] Iteration[3200/97] Loss: 1.82\n",
    "    Epoch[33] Validation MSE: 1.81 \n",
    "    Epoch[34] Validation MSE: 1.73 \n",
    "    Epoch[35] Validation MSE: 1.66 \n",
    "    Epoch[36] Validation MSE: 1.60 \n",
    "    Epoch[37] Validation MSE: 1.54 \n",
    "    Epoch[38] Iteration[3600/97] Loss: 1.60\n",
    "    Epoch[38] Validation MSE: 1.48 \n",
    "    Epoch[39] Validation MSE: 1.43 \n",
    "    Epoch[40] Validation MSE: 1.38 \n",
    "    Epoch[41] Validation MSE: 1.34 \n",
    "    Epoch[42] Iteration[4000/97] Loss: 1.27\n",
    "    Epoch[42] Validation MSE: 1.29 \n",
    "    Epoch[43] Validation MSE: 1.25 \n",
    "    Epoch[44] Validation MSE: 1.22 \n",
    "    Epoch[45] Validation MSE: 1.19 \n",
    "    Epoch[46] Iteration[4400/97] Loss: 1.11\n",
    "    Epoch[46] Validation MSE: 1.15 \n",
    "    Epoch[47] Validation MSE: 1.13 \n",
    "    Epoch[48] Validation MSE: 1.10 \n",
    "    Epoch[49] Validation MSE: 1.07 \n",
    "    Epoch[50] Iteration[4800/97] Loss: 1.11\n",
    "    Epoch[50] Validation MSE: 1.05 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    State:\n",
    "    \titeration: 4850\n",
    "    \tepoch: 50\n",
    "    \tepoch_length: 97\n",
    "    \tmax_epochs: 50\n",
    "    \toutput: 1.0818936824798584\n",
    "    \tbatch: <class 'tuple'>\n",
    "    \tmetrics: <class 'dict'>\n",
    "    \tdataloader: <class '__main__.Loader'>\n",
    "    \tseed: <class 'NoneType'>\n",
    "    \ttimes: <class 'dict'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), \"./pytorch_model\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommendation (Pytorch) Inference\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "class MF(nn.Module):\n",
    "    itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0):\n",
    "        super(MF, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def __call__(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        \n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        return total\n",
    "\n",
    "def get_top_n(model,testset,trainset,uid_input,n=10):\n",
    "    \n",
    "    preds = []\n",
    "    try:\n",
    "        uid_input = int(trainset.to_inner_uid(uid_input))\n",
    "    except KeyError:\n",
    "        return preds        \n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    for uid, iid, _ in testset: #inefficient\n",
    "        try:\n",
    "            uid_internal = int(trainset.to_inner_uid(uid))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if uid_internal==uid_input:\n",
    "            try:\n",
    "                iid_internal = int(trainset.to_inner_iid(iid))\n",
    "                movie_name = df.loc[int(iid),'name']\n",
    "                preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]])))))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones\n",
    "    if preds is not None:\n",
    "        preds.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(preds) > n:\n",
    "            preds = preds[:n]\n",
    "    return preds\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "#Data\n",
    "df = pd.read_csv('./movies.dat',sep=\"::\",header=None,engine='python')\n",
    "df.columns = ['iid','name','genre']\n",
    "df.set_index('iid',inplace=True)\n",
    "print(df.head())\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()\n",
    "```\n",
    "\n",
    "                                       name                         genre\n",
    "    iid                                                                  \n",
    "    1                      Toy Story (1995)   Animation|Children's|Comedy\n",
    "    2                        Jumanji (1995)  Adventure|Children's|Fantasy\n",
    "    3               Grumpier Old Men (1995)                Comedy|Romance\n",
    "    4              Waiting to Exhale (1995)                  Comedy|Drama\n",
    "    5    Father of the Bride Part II (1995)                        Comedy\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "#Parameters\n",
    "lr = 1e-2\n",
    "k = 10 #latent dimension\n",
    "c_bias = 1e-6\n",
    "c_vector = 1e-6\n",
    "\n",
    "model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "model.load_state_dict(torch.load('./pytorch_model'))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    MF(\n",
    "      (user): Embedding(943, 10)\n",
    "      (item): Embedding(1682, 10)\n",
    "      (bias_user): Embedding(943, 1)\n",
    "      (bias_item): Embedding(1682, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Print the recommended items for each user\n",
    "limit = 0\n",
    "for uid,_,_ in testset:\n",
    "    print('\\nUser:',uid)\n",
    "    seen = [df.loc[int(iid),'name'] for (iid, _) in trainset.ur[int(uid)]]\n",
    "    if len(seen) > 10: seen = seen[:10]\n",
    "    print('\\tSeen:',seen)\n",
    "    print('\\tRecommendations:',get_top_n(model,testset,trainset,uid,n=10))\n",
    "    limit+=1\n",
    "    if limit>3:\n",
    "        break\n",
    "```\n",
    "\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommendation (SVD) Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python\n",
    "# https://github.com/NicolasHug/Surprise\n",
    "from surprise import SVD, Dataset\n",
    "from surprise.accuracy import rmse\n",
    "from surprise.dump import dump\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Use an example algorithm: SVD.\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# predict ratings for all pairs (u, i) that are in the training set.\n",
    "testset = trainset.build_testset()\n",
    "predictions = algo.test(testset)\n",
    "rmse(predictions)                                                                              \n",
    "\n",
    "#actual predictions as thse items have not been seen by the users. there is no ground truth. \n",
    "# We predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "```\n",
    "\n",
    "    RMSE: 0.6774\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "dump('./surprise_model', predictions, algo)\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommendation (SVD) Inference\n",
    "\n",
    "\n",
    "```python\n",
    "from surprise import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "class MF(nn.Module):\n",
    "    itr = 0\n",
    "    \n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0):\n",
    "        super(MF, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def __call__(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        \n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        return total\n",
    "\n",
    "def get_top_n(model,testset,trainset,uid_input,n=10):\n",
    "    \n",
    "    preds = []\n",
    "    try:\n",
    "        uid_input = int(trainset.to_inner_uid(uid_input))\n",
    "    except KeyError:\n",
    "        return preds        \n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    for uid, iid, _ in testset: #inefficient\n",
    "        try:\n",
    "            uid_internal = int(trainset.to_inner_uid(uid))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if uid_internal==uid_input:\n",
    "            try:\n",
    "                iid_internal = int(trainset.to_inner_iid(iid))\n",
    "                movie_name = df.loc[int(iid),'name']\n",
    "                preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]])))))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones\n",
    "    if preds is not None:\n",
    "        preds.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(preds) > n:\n",
    "            preds = preds[:n]\n",
    "    return preds\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "#Data\n",
    "df = pd.read_csv('./movies.dat',sep=\"::\",header=None,engine='python')\n",
    "df.columns = ['iid','name','genre']\n",
    "df.set_index('iid',inplace=True)\n",
    "print(df.head())\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()\n",
    "```\n",
    "\n",
    "                                       name                         genre\n",
    "    iid                                                                  \n",
    "    1                      Toy Story (1995)   Animation|Children's|Comedy\n",
    "    2                        Jumanji (1995)  Adventure|Children's|Fantasy\n",
    "    3               Grumpier Old Men (1995)                Comedy|Romance\n",
    "    4              Waiting to Exhale (1995)                  Comedy|Drama\n",
    "    5    Father of the Bride Part II (1995)                        Comedy\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "#Parameters\n",
    "lr = 1e-2\n",
    "k = 10 #latent dimension\n",
    "c_bias = 1e-6\n",
    "c_vector = 1e-6\n",
    "\n",
    "model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "model.load_state_dict(torch.load('./pytorch_model'))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    MF(\n",
    "      (user): Embedding(943, 10)\n",
    "      (item): Embedding(1682, 10)\n",
    "      (bias_user): Embedding(943, 1)\n",
    "      (bias_item): Embedding(1682, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Print the recommended items for each user\n",
    "limit = 0\n",
    "for uid,_,_ in testset:\n",
    "    print('\\nUser:',uid)\n",
    "    seen = [df.loc[int(iid),'name'] for (iid, _) in trainset.ur[int(uid)]]\n",
    "    if len(seen) > 10: seen = seen[:10]\n",
    "    print('\\tSeen:',seen)\n",
    "    print('\\tRecommendations:',get_top_n(model,testset,trainset,uid,n=10))\n",
    "    limit+=1\n",
    "    if limit>3:\n",
    "        break\n",
    "```\n",
    "\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "    \n",
    "    User: 196\n",
    "    \tSeen: ['Naked Gun 33 1/3: The Final Insult (1994)', 'Free Willy (1993)', 'Rob Roy (1995)', 'Die Hard: With a Vengeance (1995)', 'Hate (Haine, La) (1995)', 'Up Close and Personal (1996)', 'Brady Bunch Movie, The (1995)', 'Miami Rhapsody (1995)', 'Baton Rouge (1988)', 'Innocents, The (1961)']\n",
    "    \tRecommendations: ['Glory (1989)', 'Losing Chase (1996)', 'Larger Than Life (1996)', 'Shadowlands (1993)', \"Pharaoh's Army (1995)\", 'Salut cousin! (1996)', 'Babyfever (1994)', 'High School High (1996)', 'Bread and Chocolate (Pane e cioccolata) (1973)', 'Rock, The (1996)']\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}