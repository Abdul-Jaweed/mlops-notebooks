{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example Online Experiment\n",
    "\n",
    "- There are many solutions, each with its own nuances. To get to a minimum viable testing setup, we will instead do the following.\n",
    "- We will develop a bare-bones setup to test two recommendation models based on the flask deployment from earlier. This will involve using the open source package `planout` (see [here](https://facebook.github.io/planout/index.html)) to do the randomization.\n",
    "- Assuming flask, surpriselib, pytorch and others are already installed, we can install `planout` using the following:\n",
    "```bash\n",
    "pip install planout\n",
    "```\n",
    "- Note that its always good to do the above in a project specific conda environment (such as `datasci-dev` we have been using before).\n",
    "- The key idea with using `planout` here is as follows. While deciding to serve the recommendations to the user when they login and reach the homepage or an appropriate screen, we will randomly pick either the pytorch model or the surpriselib model. \n",
    "- In particular, the user is assigned to a random cohort in the experiment when they login using help from `planout`.\n",
    "\n",
    "- This is accomplished by creating an instance of an appropriately defined `ModelExperiment` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model_perf_exp = ModelExperiment(userid=session['userid'])\n",
    "model_type = model_perf_exp.get('model_type')\n",
    "```\n",
    "- Here the class `ModelExperiment` is defined in a straightforward manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExperiment(SimpleExperiment):\n",
    "    def setup(self):\n",
    "            self.set_log_file('model_abtest.log')\n",
    "    def assign(self, params, userid):\n",
    "            params.use_pytorch = BernoulliTrial(p=0.5, unit=userid)\n",
    "            if params.use_pytorch:\n",
    "                    params.model_type = 'pytorch'\n",
    "            else:\n",
    "                    params.model_type = 'surprise'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The setup function defines where the log for the experiment is stored (this can be changed to a different storage location).\n",
    "- The assign function uses the user's ID to bucket them into one of the control or treatment cohorts. In our example, lets assume that the surpriselib based recommendation model is the control.\n",
    "- Take a look at the complete flask script in the code folder. What follows is a brief explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We read the models from disk and some associated metadata. Although we are loading the training data below, this is only for convenience and should not be done ideally.\n",
    "- We start with the flask app setup. Notice how the app can be configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "app.config.update(dict(\n",
    "\tDEBUG=True,\n",
    "\tSECRET_KEY='MODEL_TESTING_BY_THEJA_TULABANDHULA',\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The next few lines in the script have the recommendation function that responds with recommendations from either the surprise model or the pytorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Notice the resetting function that essentially switches the user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/reset')\n",
    "def reset():\n",
    "    session.clear()\n",
    "    return redirect(url_for('main'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Below is the rating function that documents the received rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/rate')\n",
    "def rate():\n",
    "    rate_string = request.args.get('rate')\n",
    "    try:\n",
    "            rate_val = int(rate_string)\n",
    "            assert rate_val > 0 and rate_val < 11\n",
    "\n",
    "            model_perf_exp = ModelExperiment(userid=session['userid'])\n",
    "            model_perf_exp.log_event('rate', {'rate_val': rate_val})\n",
    "\n",
    "            return render_template_string(\"\"\"\n",
    "                <html>\n",
    "                    <head>\n",
    "                        <title>Thank you for the feedback!</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "                        <p>You rating is {{ rate_val }}. Hit the back button or click below to go back to recommendations!</p>\n",
    "                        <p><a href=\"/\">Back</a></p>\n",
    "                    </body>\n",
    "                </html>\n",
    "                \"\"\", rate_val=rate_val)\n",
    "    except:\n",
    "            return render_template_string(\"\"\"\n",
    "                <html>\n",
    "                    <head>\n",
    "                        <title>Bad rating!</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "                        <p>You rating could not be parsed. That's probably not a number between 1 and 10, so we won't be accepting your rating.</p>\n",
    "                        <p><a href=\"/\">Back</a></p>\n",
    "                    </body>\n",
    "                </html>\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that currently we are logging events such as exposure (the recommendations were shown) and users explicitly rating into a simple log file in the same directory.\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:code_lecture08 theja$ ls\n",
    "flask_pytorch_model.py\tmodel_abtest.log\tmovies.dat\t\tpytorch_model\t\tsurprise_model\n",
    "flask_surprise_model.py\tmodel_testing.py\tpytorch_inference.ipynb\trecommend.py\t\ttwo_sample_test.ipynb\n",
    "(datasci-dev) ttmac:code_lecture08 theja$ head -n3 model_abtest.log\n",
    "{\"name\": \"ModelExperiment\", \"time\": 1602739669, \"salt\": \"ModelExperiment\", \"inputs\": {\"userid\": \"431\"}, \"params\": {\"use_pytorch\": 1, \"model_type\": \"pytorch\"}, \"event\": \"exposure\", \"checksum\": \"796b9a12\"}\n",
    "{\"name\": \"ModelExperiment\", \"time\": 1602739720, \"salt\": \"ModelExperiment\", \"inputs\": {\"userid\": \"431\"}, \"params\": {\"use_pytorch\": 1, \"model_type\": \"pytorch\"}, \"event\": \"exposure\", \"checksum\": \"796b9a12\"}\n",
    "{\"name\": \"ModelExperiment\", \"time\": 1602739722, \"salt\": \"ModelExperiment\", \"inputs\": {\"userid\": \"431\"}, \"params\": {\"use_pytorch\": 1, \"model_type\": \"pytorch\"}, \"event\": \"exposure\", \"checksum\": \"796b9a12\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can load this log file into a Jupyter notebook to conduct our test.\n",
    "  - [Download locally](two_sample_test.ipynb)\n",
    "\n",
    "![test1](images/stat_test1.png)\n",
    "- Lets zoom into the data we care about for testing.\n",
    "![test2](images/stat_test2.png)\n",
    "- We can do a simple two sample t test using the `scipy.stats` package.\n",
    "![test3](images/stat_test3.png)\n",
    "\n",
    "- For more deliberation on the choice of the test, have a look at [this discussion](https://stats.stackexchange.com/questions/305/when-conducting-a-t-test-why-would-one-prefer-to-assume-or-test-for-equal-vari) on the assumption about equal population variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Summary\n",
    "\n",
    " - We have seen a simple A/B testing setup where we are testing two recommendation models. Most commercial and open source implementations follow similar patterns for setting up an experiment.\n",
    " - While the basic A/B testing ideas seen above are a good way to validate improvements in models and products, more advanced techniques such as multi-armed bandits, Bayesian tests and contextual bandits may help with issues such as wrong assumptions or sample inefficiency.\n",
    " - For instance, there are several ideas worth exploring such as:\n",
    "  - combining ML withing an A/B testing setup (note that this is not related to the variants being ML models)\n",
    "  - Tests with small and large samples\n",
    "  - Repeating A/B test to address drift\n",
    "  - Avoiding peeking\n",
    "  - Dealing with multiple hypotheses\n",
    "  - Working with quantities such as posteriors rather than p-values\n",
    "  - Being careful with post-selection inference\n",
    "  - Causal modeling techniques such as causal forests and various observational techniques for causal inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
