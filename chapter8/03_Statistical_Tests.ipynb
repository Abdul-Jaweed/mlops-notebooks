{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    " - For simplicity, in this section, lets assume we are only comparing control with a single treatment/variant. If we have more variants, the underlying tests will change. \n",
    " - We will look at three different tests based on the type of question we would like to answer from a corresponding experiment.\n",
    " - These tests are:\n",
    "  - [Two-sample T Test](https://www.evanmiller.org/ab-testing/t-test.html)\n",
    "  - [Chi-squared Test](https://www.evanmiller.org/ab-testing/chi-squared.html)\n",
    "  - [Poisson Means Test](https://www.evanmiller.org/ab-testing/poisson-means.html)\n",
    " - There are always alternative tests that can be used depending on the assumptions. \n",
    "  - For instance, instead of the Poisson means test above, one could use the [Mannâ€“Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) non-parametric test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Size Considerations\n",
    "\n",
    " - Recall the terminology\n",
    " \t- Minimum detectable effect: the effect of using the new treatment/variation over a baseline/control.\n",
    " \t- Sample size per variation: the number of users that need to be exposed to the treatment and control to be able to conclusively reject or not reject the null hypothesis.\n",
    " - There are a few key choices that determine the test specs. These are:\n",
    "  - Statistical power of the test (1 -$\\beta$): which is the probability with which the the test will be able to *detect* the minimum detectable effect if it exists.\n",
    "  - Significance level ($\\alpha$): which is the probability with which the test will make a mistake and detect the minimum detectable effect when it does not exist. For instance, if both the control and the variation/treatment are the same, then there is an $\\alpha$ percent chance of detecting that the variation is better than the control!\n",
    " - Typically, in a test specifying an acceptable level of power, the minimum detectable effect size and significance determine the number of samples needed.\n",
    " - For instance, if we want to detect what we believe is a small improvement over control, we may have to test longer (i.e., test on more users) to be able to reject the null hypothesis.\n",
    " - [Demo: Evan's A/B tools](https://www.evanmiller.org/ab-testing/sample-size.html)\n",
    " - [Demo: Optimizely](https://www.optimizely.com/sample-size-calculator/)\n",
    " \t- Caveat: We are only discussing fixed horizon tests and the sample size requirements thereof. Optimizely's calculator uses a more sophisticated test suite that allows for *early stopping*. So the estimated sample sizes may differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two-sample t Test\n",
    "\n",
    "- The two-sample t test allows one to answer the following question:\n",
    "\n",
    "> Is there a difference in the average value across two groups?\n",
    "\n",
    "- We can have different null hypotheses:\n",
    " - The two means are equal\n",
    " - The mean of the first group is higher than the second\n",
    " - The mean of the second group is higher than the first\n",
    "- In this, a t-statistic based on the differences between the means of the two groups is computed.\n",
    "- Example application:\n",
    " - The sessions lengths of users on an app (e.g., Netflix)\n",
    "- [Demo](https://www.evanmiller.org/ab-testing/t-test.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-squared Test\n",
    "\n",
    "- This test allows for answering the following question:\n",
    "\n",
    "> Is there a difference in the rates of success across two groups?\n",
    "\n",
    "- The null hypothesis here is that the rates of success are equal.\n",
    "- The chi-squared statistic is computed based on the different observed success rates of the two groups.\n",
    "- Example application:\n",
    " - Successful conversion events (e.g., for everyone who downloaded an app or registered for free trials).\n",
    "- [Demo](https://www.evanmiller.org/ab-testing/chi-squared.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson Means Test\n",
    "\n",
    " - This test lets us answer the following:\n",
    "\n",
    " > Is the the rate of arrival of events (of a certain type) different across two time periods or across two groups?\n",
    "\n",
    " - The null hypothesis here is that the two rates are equal.\n",
    " - Example application:\n",
    "  - Counts of successful content interactions (e.g., news article clicks on a search page or a marketing page)\n",
    " - [Demo](https://www.evanmiller.org/ab-testing/poisson-means.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Remember?\n",
    "\n",
    " - It may seem that there are a lot of tests, each with its own assumptions about the data. That is true. But a way to approach the testing complexity is to know a few popular tests.\n",
    " - Another way is to think of tests from a linear model perspective. This is better explained at [https://lindeloev.github.io/tests-as-linear/](https://lindeloev.github.io/tests-as-linear/). The gist is that, many hypothesis tests are essentially tests for the coefficients of a corresponding linear model. With this view point, it is also easy to understand the assumptions being made. \n",
    "  - A port of the above mapping between tests and linear modeling (originally in R) to python can be viewed at [https://eigenfoo.xyz/tests-as-linear/](https://eigenfoo.xyz/tests-as-linear/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gotchas with Testing\n",
    "\n",
    " - There are many ways to make wrong inferences with A/B testing.\n",
    " - The simplest is the misunderstanding of what p-value means. P value is assigning a probability to the observed event under the distribution governed by the null hypothesis.\n",
    " - Most classical tests (at least the ones seen in introductory stats textbooks) are for a fixed horizon setting where the sample size is pre-determined. There is a big difference in assuming this as running the experiment long enough to see a statistically significant result. This latter even will happen at a much higher frequency.\n",
    "  - Stopping only when you have a significant outcome inflates the rate of false positives (by a huge margin). This error in conducting a clean experiment is called *peeking*.\n",
    " - There are adaptive testing techniques that allow one to not commit the a specific sample size in advance. These are called sequential experimental design and Bayesian experimental designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
