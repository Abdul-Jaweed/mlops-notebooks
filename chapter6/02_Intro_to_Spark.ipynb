{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Based Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Introduction\n",
    "\n",
    "#### Spark\n",
    "\n",
    " - Spark lets you run data tasks (preprocessing, feature engineering, training) on multiple machines.\n",
    " - A core idea behind spark is the notion of *resilient distributed datasets* (RDDs).\n",
    " - Using this core idea, spark is able to manage fault tolerance and scale.\n",
    " - Spark also has a abstract data type called *dataframe*, similar to pandas and R.\n",
    " - This dataframe interface sits on top of RDDs and allows for more approachable specification of our tasks.\n",
    " - The the name of RDD implies, we are primarily looking at data volumes much bigger than what a typical single machine can handle.\n",
    " \t- Single machines already have ~100 gigs of RAM and 12-32 cores for compute.\n",
    " - Very popular in multiple industries and thousands of companies.\n",
    " \t- Can be seen as a successor to Hadoop and MapReduce systems (technically it is a part of Hadoop)\n",
    "\n",
    "#### PySpark\n",
    "\n",
    " - PySpark is a way to access spark using Python.\n",
    " - As much of the work in ML is being done in Python, this is a very useful tool to scale to a large data volume.\n",
    " - And that scaling can be achieved without a lot of engineering effort.\n",
    "\n",
    "\n",
    "#### Spark vs Container Deployments\n",
    "\n",
    " - We have looked at both hosted and managed deployments of model training and serving via containerization/docker, serverless techniques, kubernetes, airflow etc.\n",
    " - The container that we built have to run on underlying nodes in isolation. That is, the training code cannot spread across machines.\n",
    " \t- And we need this if the data to train on is very large.\n",
    " - Spark lets us span our tasks across nodes for both training and serving, which is a distinct advantage over containerized training pipelines in certain business scenarios.\n",
    "\n",
    "\n",
    "#### Our Goals\n",
    "\n",
    " - We will go through the basics of PySpark dataframes.\n",
    " \t- We will also learn Pandas user defined functions (UDFs) and how they help work with PySpark dataframes and large data with ease.\n",
    " - Use it to design tasks and pipelines, and also take deployment into consideration.\n",
    " \t- For instance, we will look at how to get data from AWS S3 read by the spark cluster, as well as how to write data from spark to S3.\n",
    " - See how to use PySpark on the cloud (AWS/GCP).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
