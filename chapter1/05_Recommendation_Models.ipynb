{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Recommendation Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will look at two models for recommending movies to existing users. \n",
    "\n",
    " - Matrix factorization based on the surprise package.\n",
    " - Matrix factorization based on Pytorch.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommendation (Pytorch) Training\n",
    "\n",
    "Assuming we have installed pytorch (in the previous section), lets install certain additional packages. In particular, the package `surprise` can be installed using the command `conda install -c conda-forge scikit-surprise` (ensure you are not in the base environment but in the datasci-dev or some other specific environment).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset #can be replaced by explicitly importing the movielens data, https://github.com/NicolasHug/Surprise\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader():\n",
    "    current = 0\n",
    "\n",
    "    def __init__(self, x, y, batchsize=1024, do_shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batchsize = batchsize\n",
    "        self.batches = range(0, len(self.y), batchsize)\n",
    "        if do_shuffle:\n",
    "            # Every epoch re-shuffle the dataset\n",
    "            self.x, self.y = shuffle(self.x, self.y)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Reset & return a new iterator\n",
    "        self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches\n",
    "        return int(len(self.x) / self.batchsize)\n",
    "\n",
    "    def __next__(self):\n",
    "        n = self.batchsize\n",
    "        if self.current + n >= len(self.y):\n",
    "            raise StopIteration\n",
    "        i = self.current\n",
    "        xs = torch.from_numpy(self.x[i:i + n])\n",
    "        ys = torch.from_numpy(self.y[i:i + n])\n",
    "        self.current += n\n",
    "        return (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "\n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0):\n",
    "        super(MF, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "\n",
    "        def l2_regularize(array):\n",
    "            loss = torch.sum(array**2)\n",
    "            return loss\n",
    "\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        \n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration: 1. training loss: 20.42420768737793\n",
      "iteration: 2. training loss: 17.85934066772461\n",
      "iteration: 3. training loss: 15.529234886169434\n",
      "iteration: 4. training loss: 14.985013961791992\n",
      "iteration: 5. training loss: 13.329107284545898\n",
      "iteration: 6. training loss: 11.790022850036621\n",
      "iteration: 7. training loss: 11.360279083251953\n",
      "iteration: 8. training loss: 10.009636878967285\n",
      "iteration: 9. training loss: 8.997058868408203\n",
      "iteration: 10. training loss: 8.372835159301758\n",
      "iteration: 11. training loss: 7.8731465339660645\n",
      "iteration: 12. training loss: 7.506683349609375\n",
      "iteration: 13. training loss: 6.360579013824463\n",
      "iteration: 14. training loss: 6.4721760749816895\n",
      "iteration: 15. training loss: 5.841653347015381\n",
      "iteration: 16. training loss: 5.418582439422607\n",
      "iteration: 17. training loss: 4.895118236541748\n",
      "iteration: 18. training loss: 4.843044281005859\n",
      "iteration: 19. training loss: 4.418150424957275\n",
      "iteration: 20. training loss: 4.507508754730225\n",
      "iteration: 21. training loss: 4.566267967224121\n",
      "iteration: 22. training loss: 3.958303451538086\n",
      "iteration: 23. training loss: 4.012577533721924\n",
      "iteration: 24. training loss: 3.56540846824646\n",
      "iteration: 25. training loss: 3.6683943271636963\n",
      "iteration: 26. training loss: 3.51039719581604\n",
      "iteration: 27. training loss: 2.96724271774292\n",
      "iteration: 28. training loss: 2.791757345199585\n",
      "iteration: 29. training loss: 2.3587698936462402\n",
      "iteration: 30. training loss: 2.6218504905700684\n",
      "iteration: 31. training loss: 2.699833393096924\n",
      "iteration: 32. training loss: 2.8032643795013428\n",
      "iteration: 33. training loss: 2.4010236263275146\n",
      "iteration: 34. training loss: 2.293672561645508\n",
      "iteration: 35. training loss: 2.1884400844573975\n",
      "iteration: 36. training loss: 2.4400599002838135\n",
      "iteration: 37. training loss: 1.998067021369934\n",
      "iteration: 38. training loss: 1.9955096244812012\n",
      "iteration: 39. training loss: 1.785199522972107\n",
      "iteration: 40. training loss: 1.7962857484817505\n",
      "iteration: 41. training loss: 1.8536224365234375\n",
      "iteration: 42. training loss: 1.9599153995513916\n",
      "iteration: 43. training loss: 1.684167504310608\n",
      "iteration: 44. training loss: 1.69539213180542\n",
      "iteration: 45. training loss: 1.5264830589294434\n",
      "iteration: 46. training loss: 1.5848523378372192\n",
      "iteration: 47. training loss: 1.6371634006500244\n",
      "iteration: 48. training loss: 1.7853703498840332\n",
      "iteration: 49. training loss: 1.7348185777664185\n",
      "iteration: 50. training loss: 1.6429839134216309\n",
      "iteration: 51. training loss: 1.3848907947540283\n",
      "iteration: 52. training loss: 1.6437842845916748\n",
      "iteration: 53. training loss: 1.3430677652359009\n",
      "iteration: 54. training loss: 1.5688366889953613\n",
      "iteration: 55. training loss: 1.3998985290527344\n",
      "iteration: 56. training loss: 1.2875908613204956\n",
      "iteration: 57. training loss: 1.268955945968628\n",
      "iteration: 58. training loss: 1.4192450046539307\n",
      "iteration: 59. training loss: 1.1934247016906738\n",
      "iteration: 60. training loss: 1.2168350219726562\n",
      "iteration: 61. training loss: 1.3739895820617676\n",
      "iteration: 62. training loss: 1.3168914318084717\n",
      "iteration: 63. training loss: 1.2484146356582642\n",
      "iteration: 64. training loss: 1.2823705673217773\n",
      "iteration: 65. training loss: 1.1760225296020508\n",
      "iteration: 66. training loss: 1.2893379926681519\n",
      "iteration: 67. training loss: 1.2031095027923584\n",
      "iteration: 68. training loss: 1.1515841484069824\n",
      "iteration: 69. training loss: 1.2792245149612427\n",
      "iteration: 70. training loss: 1.216879963874817\n",
      "iteration: 71. training loss: 1.0835596323013306\n",
      "iteration: 72. training loss: 1.2529200315475464\n",
      "iteration: 73. training loss: 1.2380620241165161\n",
      "iteration: 74. training loss: 1.0779699087142944\n",
      "iteration: 75. training loss: 1.128207802772522\n",
      "iteration: 76. training loss: 1.1838161945343018\n",
      "iteration: 77. training loss: 1.0952773094177246\n",
      "iteration: 78. training loss: 1.0703791379928589\n",
      "iteration: 79. training loss: 1.219058871269226\n",
      "iteration: 80. training loss: 1.1232625246047974\n",
      "iteration: 81. training loss: 1.1233044862747192\n",
      "iteration: 82. training loss: 1.1426836252212524\n",
      "iteration: 83. training loss: 1.190811276435852\n",
      "iteration: 84. training loss: 1.1612569093704224\n",
      "iteration: 85. training loss: 1.0748461484909058\n",
      "iteration: 86. training loss: 1.1608384847640991\n",
      "iteration: 87. training loss: 1.0737138986587524\n",
      "iteration: 88. training loss: 1.103972315788269\n",
      "iteration: 89. training loss: 1.1376703977584839\n",
      "iteration: 90. training loss: 1.1402852535247803\n",
      "iteration: 91. training loss: 1.1081783771514893\n",
      "iteration: 92. training loss: 0.9870721101760864\n",
      "iteration: 93. training loss: 1.0817840099334717\n",
      "iteration: 94. training loss: 1.0319467782974243\n",
      "iteration: 95. training loss: 1.011339545249939\n",
      "iteration: 96. training loss: 1.1939691305160522\n",
      "iteration: 97. training loss: 1.1700149774551392\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #Data\n",
    "    data = Dataset.load_builtin('ml-100k')\n",
    "    trainset = data.build_full_trainset()\n",
    "    uir = np.array([x for x in trainset.all_ratings()])\n",
    "    train_x = test_x = uir[:,:2].astype(np.int64) #for simplicity\n",
    "    train_y = test_y = uir[:,2].astype(np.float32)\n",
    "\n",
    "    #Parameters\n",
    "    lr = 1e-1\n",
    "    k = 10 #latent dimension\n",
    "    c_bias = 1e-6\n",
    "    c_vector = 1e-6\n",
    "    batchsize = 1024\n",
    "\n",
    "    model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    dataloader = Loader(train_x, train_y, batchsize=batchsize)\n",
    "\n",
    "    itr = 0\n",
    "    for batch in dataloader:\n",
    "        itr += 1\n",
    "        prediction = model(batch[0])\n",
    "        loss = model.loss(prediction,batch[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"iteration: {itr}. training loss: {loss}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"./recommendation_model_pytorch.pkl\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "source": [
    "### Recommendation (Pytorch) Inference\n",
    "\n",
    "For inference, we will need most of the code from before (especially the model definition)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from recommend_pytorch_base import MF\n",
    "from surprise import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "def get_top_n(model,testset,trainset,uid_input,movies_df,n=10):\n",
    "    \n",
    "    preds = []\n",
    "    try:\n",
    "        uid_input = int(trainset.to_inner_uid(uid_input))\n",
    "    except KeyError:\n",
    "        return preds        \n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    for uid, iid, _ in testset: #inefficient\n",
    "        try:\n",
    "            uid_internal = int(trainset.to_inner_uid(uid))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if uid_internal==uid_input:\n",
    "            try:\n",
    "                iid_internal = int(trainset.to_inner_iid(iid))\n",
    "                movie_name = movies_df.loc[int(iid),'name']\n",
    "                preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]])))))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones\n",
    "    if preds is not None:\n",
    "        preds.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(preds) > n:\n",
    "            preds = preds[:n]\n",
    "    return preds\n",
    "\n",
    "def get_previously_seen(trainset, uid, movies_df):\n",
    "    seen = []\n",
    "    for (iid, _) in trainset.ur[int(uid)]:\n",
    "        try:\n",
    "            seen.append(movies_df.loc[int(iid),'name'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if len(seen) > 10:\n",
    "            break\n",
    "    return seen\n",
    "\n",
    "def main():\n",
    "    #Data\n",
    "    movies_df = pd.read_csv('./movies.dat',sep=\"::\",header=None,engine='python')\n",
    "    movies_df.columns = ['iid','name','genre']\n",
    "    movies_df.set_index('iid',inplace=True)\n",
    "    data = Dataset.load_builtin('ml-1m')\n",
    "    trainset = data.build_full_trainset()\n",
    "    testset = trainset.build_anti_testset()\n",
    "\n",
    "    k = 10 #latent dimension\n",
    "    c_bias = 1e-6\n",
    "    c_vector = 1e-6\n",
    "\n",
    "    model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "    model.load_state_dict(torch.load('./recommendation_model_pytorch.pkl'))\n",
    "    model.eval()\n",
    "\n",
    "    # Print the recommended items for sample users\n",
    "    sample_users = list(set([x[0] for x in testset]))[:4]\n",
    "\n",
    "    for uid in sample_users:\n",
    "        \n",
    "        print('User:',uid)\n",
    "        print('\\n')\n",
    "\n",
    "        print('\\tSeen:')\n",
    "        seen = get_previously_seen(trainset, uid, movies_df)\n",
    "        pprint.pprint(seen)\n",
    "        print('\\n')\n",
    "\n",
    "        print('\\tRecommendations:')\n",
    "        recommended = get_top_n(model, testset, trainset, uid, movies_df, n=10)\n",
    "        pprint.pprint([x[1] for x in recommended])\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Recommendation (SVD) Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# https://github.com/NicolasHug/Surprise\n",
    "from surprise import SVD, Dataset\n",
    "from surprise.accuracy import rmse\n",
    "from surprise.dump import dump\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-1m')\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Use an example algorithm: SVD.\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# predict ratings for all pairs (u, i) that are in the training set.\n",
    "testset = trainset.build_testset()\n",
    "predictions = algo.test(testset)\n",
    "rmse(predictions)                                                                              \n",
    "\n",
    "#actual predictions as thse items have not been seen by the users. there is no ground truth. \n",
    "# We predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "dump('./recommendation_model_surprise.pkl', predictions, algo)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Recommendation (SVD) Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/NicolasHug/Surprise\n",
    "from surprise import SVD, Dataset\n",
    "from surprise.dump import load\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from recommend_pytorch_inf import get_previously_seen\n",
    "\n",
    "def get_top_n_all(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    movies_df = pd.read_csv('./movies.dat',sep=\"::\",header=None,engine='python')\n",
    "    movies_df.columns = ['iid','name','genre']\n",
    "    movies_df.set_index('iid',inplace=True)\n",
    "    predictions, algo = load('./recommendation_model_surprise.pkl')\n",
    "\n",
    "\n",
    "    top_n = get_top_n_all(predictions, n=10)\n",
    "    # Print the recommended items for some sample users\n",
    "\n",
    "    itr = 0\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        itr += 1\n",
    "        if itr == 5:\n",
    "            break\n",
    "\n",
    "        print('User:',uid)\n",
    "        print('\\n')\n",
    "\n",
    "        print('\\tSeen:')\n",
    "        seen = get_previously_seen(algo.trainset, uid, movies_df)\n",
    "\n",
    "        pprint.pprint(seen)\n",
    "        print('\\n')\n",
    "\n",
    "        print('\\tRecommendations:')\n",
    "        recommended = [movies_df.loc[int(iid),'name'] for (iid, _) in user_ratings]\n",
    "        pprint.pprint(recommended)\n",
    "        print('\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ]
}