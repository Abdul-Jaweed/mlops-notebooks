{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Recommendation Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will look at a pytorch based model for recommending movies to existing users. This model is based on the idea of *matrix factorization based collaborative filtering*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommendation (Pytorch) Training\n",
    "\n",
    "Assuming we have installed pytorch (in the previous section), lets install certain additional packages. In particular, the package `surprise` can be installed using the command `conda install -c conda-forge scikit-surprise` (ensure you are not in the base environment but in the datasci-dev or some other specific environment). This package provides a convenient method to get access to the movie recommendation dataset.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset #can be replaced by explicitly importing the movielens data, https://github.com/NicolasHug/Surprise\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader():\n",
    "    current = 0\n",
    "\n",
    "    def __init__(self, x, y, batchsize=1024, do_shuffle=True):\n",
    "        self.shuffle = shuffle\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batchsize = batchsize\n",
    "        self.batches = range(0, len(self.y), batchsize)\n",
    "        if do_shuffle:\n",
    "            # Every epoch re-shuffle the dataset\n",
    "            self.x, self.y = shuffle(self.x, self.y)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Reset & return a new iterator\n",
    "        self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
    "        self.current = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of batches\n",
    "        return int(len(self.x) / self.batchsize)\n",
    "\n",
    "    def __next__(self):\n",
    "        n = self.batchsize\n",
    "        if self.current + n >= len(self.y):\n",
    "            raise StopIteration\n",
    "        i = self.current\n",
    "        xs = torch.from_numpy(self.x[i:i + n])\n",
    "        ys = torch.from_numpy(self.y[i:i + n])\n",
    "        self.current += n\n",
    "        return (xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "\n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, c_bias=1.0):\n",
    "        super(MF, self).__init__()\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_bias = c_bias\n",
    "        self.c_vector = c_vector\n",
    "        \n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, train_x):\n",
    "        user_id = train_x[:, 0]\n",
    "        item_id = train_x[:, 1]\n",
    "        vector_user = self.user(user_id)\n",
    "        vector_item = self.item(item_id)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        \n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "\n",
    "        def l2_regularize(array):\n",
    "            loss = torch.sum(array**2)\n",
    "            return loss\n",
    "\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        \n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        \n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        total = loss_mse + prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Data\n",
    "    data = Dataset.load_builtin('ml-100k')\n",
    "    trainset = data.build_full_trainset()\n",
    "    uir = np.array([x for x in trainset.all_ratings()])\n",
    "    train_x = test_x = uir[:,:2].astype(np.int64) #for simplicity\n",
    "    train_y = test_y = uir[:,2].astype(np.float32)\n",
    "\n",
    "    #Parameters\n",
    "    lr = 1e-1\n",
    "    k = 10 #latent dimension\n",
    "    c_bias = 1e-6\n",
    "    c_vector = 1e-6\n",
    "    batchsize = 1024\n",
    "\n",
    "    model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    dataloader = Loader(train_x, train_y, batchsize=batchsize)\n",
    "\n",
    "    itr = 0\n",
    "    for batch in dataloader:\n",
    "        itr += 1\n",
    "        prediction = model(batch[0])\n",
    "        loss = model.loss(prediction,batch[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"iteration: {itr}. training loss: {loss}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"./recommendation_model_pytorch.pkl\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "source": [
    "### Recommendation (Pytorch) Inference\n",
    "\n",
    "For inference, we will use most of the code from before (especially the model definition). We will see later that this may not be resource efficient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from recommend_pytorch_base import MF\n",
    "from surprise import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "def get_top_n(model,testset,trainset,uid_input,movies_df,n=10):\n",
    "    \n",
    "    preds = []\n",
    "    try:\n",
    "        uid_input = int(trainset.to_inner_uid(uid_input))\n",
    "    except KeyError:\n",
    "        return preds        \n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    for uid, iid, _ in testset: #inefficient\n",
    "        try:\n",
    "            uid_internal = int(trainset.to_inner_uid(uid))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if uid_internal==uid_input:\n",
    "            try:\n",
    "                iid_internal = int(trainset.to_inner_iid(iid))\n",
    "                movie_name = movies_df.loc[int(iid),'name']\n",
    "                preds.append((iid,movie_name,float(model(torch.tensor([[uid_input,iid_internal]])))))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones\n",
    "    if preds is not None:\n",
    "        preds.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(preds) > n:\n",
    "            preds = preds[:n]\n",
    "    return preds\n",
    "\n",
    "def get_previously_seen(trainset, uid, movies_df):\n",
    "    seen = []\n",
    "    for (iid, _) in trainset.ur[int(uid)]:\n",
    "        try:\n",
    "            seen.append(movies_df.loc[int(iid),'name'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if len(seen) > 10:\n",
    "            break\n",
    "    return seen\n",
    "\n",
    "def main():\n",
    "    #Data\n",
    "    movies_df = pd.read_csv('./movies.dat',sep=\"::\",header=None,engine='python')\n",
    "    movies_df.columns = ['iid','name','genre']\n",
    "    movies_df.set_index('iid',inplace=True)\n",
    "    data = Dataset.load_builtin('ml-1m')\n",
    "    trainset = data.build_full_trainset()\n",
    "    testset = trainset.build_anti_testset()\n",
    "\n",
    "    k = 10 #latent dimension\n",
    "    c_bias = 1e-6\n",
    "    c_vector = 1e-6\n",
    "\n",
    "    model = MF(trainset.n_users, trainset.n_items, k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "    model.load_state_dict(torch.load('./recommendation_model_pytorch.pkl'))\n",
    "    model.eval()\n",
    "\n",
    "    # Print the recommended items for sample users\n",
    "    sample_users = list(set([x[0] for x in testset]))[:4]\n",
    "\n",
    "    for uid in sample_users:\n",
    "        \n",
    "        print('User:',uid)\n",
    "        print('\\n')\n",
    "\n",
    "        print('\\tSeen:')\n",
    "        seen = get_previously_seen(trainset, uid, movies_df)\n",
    "        pprint.pprint(seen)\n",
    "        print('\\n')\n",
    "\n",
    "        print('\\tRecommendations:')\n",
    "        recommended = get_top_n(model, testset, trainset, uid, movies_df, n=10)\n",
    "        pprint.pprint([x[1] for x in recommended])\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}