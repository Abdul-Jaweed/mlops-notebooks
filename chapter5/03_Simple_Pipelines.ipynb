{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "- Here are some common task blocks:\n",
    "  - Extract data and process it\n",
    "  - Train a model\n",
    "  - Predict on a test set\n",
    "  - Save results in a database\n",
    "\n",
    "\n",
    " - The data must first be prepared (via [ETL](https://en.wikipedia.org/wiki/Extract%2C_transform%2C_load)/ELT or extract/transform/load jobs). \n",
    " - Training and making predictions (i.e., inference) requires appropriate compute resources.\n",
    " - Data read and write imply access to an external service (such as an application database) or storage (such as AWS S3).\n",
    "   - When we do data science work on a local machine, we may likely use some simple/manual ways to read data (likely from disk or from databases) as well as write our results to disk. Unfortunately, this is not sufficient in a production setting or a business context. In these latter contexts, automation, reliability, maintenance/resource requirements and documentation become paramount.\n",
    "\n",
    "\n",
    " - In all setting, the aforementioned jobs may need to run periodically so as to get the latest data from logging systems/data lakes and to make the freshest predictions.\n",
    "\n",
    " - Some example of pipelines are:\n",
    "  - persistent model pipelines: model update is de-coupled from updating predictions. For instance, the model is updated monthly, but predictions are made in real-time or daily/hourly.\n",
    "  - transient model pipelines: model update is tightly coupled with predictions. This helps with ensuring that the model is not losing prediction accuracy due to changing data distributions (e.g., time varying).\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transient Pipeline\n",
    "\n",
    " - We will build a transient pipeline such that the model is built from scratch every-time to generate predictions.\n",
    "  \t- This will need compute resources (GPUs if its a neural network), which may impact cost benefit analysis in a business context.\n",
    "\n",
    " - Our sub-tasks in the above transient pipeline are as follows:\n",
    "  - get the training data\n",
    "    - We have used surpriselib's Dataset class to get the movielens 100k/1m dataset versions.\n",
    "  - train the model\n",
    "    - We have traine a recommendation engine for movie recommendation using pytorch (we have seen this model before) using the movielens data.\n",
    "  - use the model for predictions\n",
    "    - We have a script that reads a model from disk and makes predictions for a given set of users.\n",
    "  - save the results in an external resource. In particular, we will try out [BigQuery](https://cloud.google.com/bigquery/).\n",
    "    - This is new, and we illustrate a way to do this below.\n",
    "\n",
    " - In the below script we are peforming the last two steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommend_pytorch_train import MF\n",
    "from recommend_pytorch_inf import get_top_n\n",
    "import torch\n",
    "import pandas as pd\n",
    "import surprise\n",
    "import datetime\n",
    "import time\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "\n",
    "def get_model_from_disk():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # data preload\n",
    "    data = surprise.Dataset.load_builtin('ml-1m')\n",
    "    trainset = data.build_full_trainset()\n",
    "    testset = trainset.build_anti_testset()\n",
    "    movies_df = pd.read_csv('../data/ml-1m/movies.dat',\n",
    "                            sep=\"::\", header=None, engine='python')\n",
    "    movies_df.columns = ['iid', 'name', 'genre']\n",
    "    movies_df.set_index('iid', inplace=True)\n",
    "\n",
    "    # model preload\n",
    "    k = 100  # latent dimension\n",
    "    c_bias = 1e-6\n",
    "    c_vector = 1e-6\n",
    "    model = MF(trainset.n_users, trainset.n_items,\n",
    "               k=k, c_bias=c_bias, c_vector=c_vector)\n",
    "    model.load_state_dict(torch.load(\n",
    "        '../data/models/recommendation_model_pytorch.pkl'))  # TODO: prevent overwriting\n",
    "    model.eval()\n",
    "\n",
    "    print('Model and data preloading completed in ', time.time()-start_time)\n",
    "\n",
    "    return model, testset, trainset, movies_df\n",
    "\n",
    "\n",
    "def get_predictions(model, testset, trainset, movies_df):\n",
    "\n",
    "    # save the recommended items for a given set of users\n",
    "    sample_users = list(set([x[0] for x in testset]))[:4]\n",
    "\n",
    "\n",
    "    df_list = []\n",
    "    for uid in sample_users:\n",
    "        recommended = get_top_n(model, testset, trainset, uid, movies_df, n=10)\n",
    "        df_list.append(pd.DataFrame(data={'uid':[uid]*len(recommended),\n",
    "                                    'recommended': [x[1] for x in recommended]},\n",
    "            columns=['uid','recommended']))\n",
    "\n",
    "    df = pd.concat(df_list, sort=False)\n",
    "    df['pred_time'] = str(datetime.datetime.now())\n",
    "    return df\n",
    "\n",
    "def upload_to_bigquery(df):\n",
    "    #Send predictions to BigQuery\n",
    "    #requires a credential file in the current working directory\n",
    "    table_id = \"movie_recommendation_service.predicted_movies\"\n",
    "    project_id = \"authentic-realm-276822\"\n",
    "    credentials = service_account.Credentials.from_service_account_file('model-user.json')\n",
    "    pandas_gbq.to_gbq(df, table_id, project_id=project_id, if_exists = 'replace', credentials=credentials)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model, testset, trainset, movies_df = get_model_from_disk()\n",
    "    df = get_predictions(model, testset, trainset, movies_df)\n",
    "    print(df)\n",
    "    upload_to_bigquery(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - To get the above code running, we will install one additional package called `pandas_gbq` from [https://pandas-gbq.readthedocs.io/en/latest/](https://pandas-gbq.readthedocs.io/en/latest/) to upload our predictions to Google's BigQuery managed service (can act like an application database).\n",
    "\n",
    "```bash\n",
    "conda install pandas-gbq --channel conda-forge\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - (Aside) To do a quick check if you are authenticated, execute the following commands in the terminal (don't forget to set the environment variable using `export GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/model-user.json` beforehand):\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:pipelines theja$ gcloud auth list\n",
    "   Credentialed Accounts\n",
    "ACTIVE  ACCOUNT\n",
    "*       *****@gmail.com\n",
    "\n",
    "To set the active account, run:\n",
    "    $ gcloud config set account `ACCOUNT`\n",
    "\n",
    "(datasci-dev) ttmac:pipelines theja$ gcloud config list project\n",
    "[core]\n",
    "project = authentic-realm-276822\n",
    "\n",
    "Your active configuration is: [default]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - (Aside) You may need to do a downgrade of a package using the command `conda install google-cloud-core==1.3.0` in case you are seeing errors such as\n",
    "\n",
    "```python\n",
    "AttributeError: 'ClientOptions' object has no attribute 'scopes'\n",
    "```\n",
    "\n",
    " - Once we run all cells of the notebook, we have essentially pushed a pandas dataframe of predictions to Google BigQuery.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - We should be able to see these predictions on the Google cloud console. So lets open up the Google Console homepage.\n",
    "\n",
    "![bq1](images/bq1.png)\n",
    "\n",
    "\n",
    " - From the console homepage, navigating to BigQuery lands us the following page. \n",
    "\n",
    "![bq2](images/bq2.png)\n",
    "\n",
    "\n",
    " - We are not interested in the SQL editor at the moment. At the bottom right, we can see our project.\n",
    "\n",
    "![bq3](images/bq3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Expand the project on the left column to get to the `movie_recommendation_service` database and then to the `predicted_movies` table. The default information is the schema.\n",
    "\n",
    "![bq4](images/bq4.png)\n",
    "\n",
    " - Changing from the scema tab to the preview tab shows that the upload was successful.\n",
    "\n",
    "![bq5](images/bq5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Lets rerun the script from the commandline. I am assuming that the `model-user.json` is in the current directory for simplicity. If its inconvenient to place it in the current directory, we can set the environment variable `GOOGLE_APPLICATION_CREDENTIALS`.\n",
    "\n",
    " - Going back to the BigQuery interface, the only thing that has changed is the timestamp when the predictions were generated (previewed results may not retrieve the same user-ids).\n",
    "\n",
    " ![bq6](images/bq6.png)\n",
    "\n",
    " - Querying from this table can also be done from a notebook using the following snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4aa3f95a3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"select * from movie_recommendation_service.predicted_movies\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/bin/miniconda3/envs/datasci-dev/lib/python3.8/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, project, credentials, _http, location, default_query_job_config, client_info, client_options)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mclient_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     ):\n\u001b[0;32m--> 183\u001b[0;31m         super(Client, self).__init__(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/bin/miniconda3/envs/datasci-dev/lib/python3.8/site-packages/google/cloud/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, project, credentials, client_options, _http)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_http\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0m_ClientProjectMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_http\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_http\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/bin/miniconda3/envs/datasci-dev/lib/python3.8/site-packages/google/cloud/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, project)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             raise EnvironmentError(\n",
      "\u001b[0;32m~/local/bin/miniconda3/envs/datasci-dev/lib/python3.8/site-packages/google/cloud/client.py\u001b[0m in \u001b[0;36m_determine_default\u001b[0;34m(project)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_determine_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;34m\"\"\"Helper:  use default project detection.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_determine_default_project\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/bin/miniconda3/envs/datasci-dev/lib/python3.8/site-packages/google/cloud/_helpers.py\u001b[0m in \u001b[0;36m_determine_default_project\u001b[0;34m(project)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \"\"\"\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/bin/miniconda3/envs/datasci-dev/lib/python3.8/site-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_HELP_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './model-user.json'\n",
    "client = bigquery.Client()\n",
    "sql = \"select * from movie_recommendation_service.predicted_movies\"\n",
    "df = client.query(sql).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark\n",
    "\n",
    " - While we did all four blocks in a couple of scripts, it makes sense to break the whole operation into 4 explicit blocks (fetch data, train, predict, send predictions to database service). This, way a block can retry its execution if it fails and can also be manually handled by a team member. In particular, this retry can be automated using scheduling tools. One such tool is `cron`, which is the subject of the next section. \n",
    "\n",
    " - For simplicity we will containerize the inference and sending predictions to database blocks of this transient model pipeline into a single container and script. Our next tool will allow us to run it automatically in a periodic manner.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
