{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "- What are some common task blocks?\n",
    "  - Extract data\n",
    "  - Train a model\n",
    "  - Predict on a test set\n",
    "  - Save results in a database\n",
    "\n",
    "\n",
    " - The data must first be prepared (via [ETL](https://en.wikipedia.org/wiki/Extract%2C_transform%2C_load) or extract/transform/load jobs). \n",
    " - Training and making predictions requires appropriate compute resources.\n",
    " - Data read and write imply access to an external service (such as a database) or storage (such as AWS S3).\n",
    "   - When you do data science work on a local machine, you will likely use some simple ways to read data (likely from disk or from databases) as well as write your results to disk. But this is not sufficient in a production setting.\n",
    "\n",
    "\n",
    " - And these jobs may need to run periodically so as to get the latest data from logging systems/data lakes.\n",
    "\n",
    " - Some example of batch pipelines are:\n",
    "  - persistent model pipelines: model update is de-coupled from updating predictions.\n",
    "  - transient model pipelines: model update is tightly coupled with predictions. This helps with ensuring that the model is not losing prediction accuracy due to changing data distributions (e.g., time varying).\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transient Pipeline\n",
    "\n",
    " - We will build a pipeline such that it is built from scratch every-time to generate predictions.\n",
    "  \t- This will need compute resources (GPUs if its a neural network), which may impact cost benefit analysis.\n",
    "\n",
    " - Our sub-tasks are as follows:\n",
    "  - get the training data\n",
    "  - train a model\n",
    "  - use the model for predictions\n",
    "  - save the results in an external resource. In particular, we will try out [BigQuery](https://cloud.google.com/bigquery/).\n",
    "\n",
    "\n",
    " - We will be training a recommendation engine for movie recommendation using the `lightfm` [package](https://github.com/lyst/lightfm). In particular, we will follow the example [here](https://github.com/lyst/lightfm/blob/master/examples/quickstart/quickstart.ipynb).\n",
    "\n",
    "![notebook1](images/notebook1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " - Lets clone the repository and start a local jupyter notebook server to have a look at the notebook. For cloning we use the following:\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:pipelines theja$ git clone https://github.com/lyst/lightfm.git\n",
    "```\n",
    "\n",
    "![notebook2](images/notebook2.png)\n",
    "\n",
    " - Alternatively, we can load the notebook on Google colab. For this, navigate to [colab.research.google.com](colab.research.google.com) and logging in using your google account.\n",
    "\n",
    "![colab1](images/colab1.png)\n",
    "\n",
    " - You can choose the *Github* tab and paste the url to the repository.\n",
    "\n",
    " ![colab2](images/colab2.png)\n",
    "\n",
    " - Colab will open the notebook for you. \n",
    "\n",
    " ![colab3](images/colab3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - The colab notebook above is not stored on your google drive yet. You will need to explicitly choose so (use the `file->save a copy in drive`). You can also turn on GPU option (not needed for this notebook) by navigating to `runtime-> change runtime type` as shown below.\n",
    "\n",
    "  ![colab4a](images/colab4a.png)\n",
    "\n",
    "  ![colab4b](images/colab4b.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Lets come back to our local jupyter notebook. We can actually execute the notebook by running the following command. See [https://github.com/jupyter/nbconvert/issues/515](https://github.com/jupyter/nbconvert/issues/515) for more information on the command line arguments used here.\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:pipelines theja$ cd lightfm/examples/quickstart/\n",
    "(datasci-dev) ttmac:quickstart theja$ jupyter nbconvert --to notebook --ExecutePreprocessor.kernel_name=python3 --inplace --execute quickstart.ipynb\n",
    "[NbConvertApp] Converting notebook quickstart.ipynb to notebook\n",
    "[NbConvertApp] Executing notebook with kernel: python3\n",
    "[NbConvertApp] Writing 8387 bytes to quickstart.ipynb\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - We will slightly modify the quickstart notebook to delineate the four steps and add the BigQuery export code.\n",
    "   - [Download notebook locally](recommend_lightfm.ipynb)\n",
    "   - Open with Colab bu using the URL [https://github.com/ChicagoDataScience/MLOps/](https://github.com/ChicagoDataScience/MLOps/).\n",
    "\n",
    " - We will install one additional package called `pandas_gbq` from [https://pandas-gbq.readthedocs.io/en/latest/](https://pandas-gbq.readthedocs.io/en/latest/) to upload our predictions to Google's BigQuery managed service (can act like an application database).\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:quickstart theja$ conda install pandas-gbq --channel conda-forge\n",
    "Collecting package metadata (current_repodata.json): done\n",
    "Solving environment: done\n",
    ".\n",
    ".\n",
    "(truncated)\n",
    ".\n",
    ".\n",
    "oauthlib-3.0.1       | 82 KB     | ############################################################################################################ | 100%\n",
    "Preparing transaction: done\n",
    "Verifying transaction: done\n",
    "Executing transaction: done\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - (Aside) To do a quick check if you are authenticated, execute the following commands in the terminal (don't forget to set the environment variable using `export GOOGLE_APPLICATION_CREDENTIALS=/Users/theja/model-user.json` beforehand):\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:pipelines theja$ gcloud auth list\n",
    "   Credentialed Accounts\n",
    "ACTIVE  ACCOUNT\n",
    "*       *****@gmail.com\n",
    "\n",
    "To set the active account, run:\n",
    "    $ gcloud config set account `ACCOUNT`\n",
    "\n",
    "(datasci-dev) ttmac:pipelines theja$ gcloud config list project\n",
    "[core]\n",
    "project = authentic-realm-276822\n",
    "\n",
    "Your active configuration is: [default]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - (Aside) You may need to do a downgrade of a package using the command `conda install google-cloud-core==1.3.0` in case you are seeing errors such as\n",
    "\n",
    "```python\n",
    "AttributeError: 'ClientOptions' object has no attribute 'scopes'\n",
    "```\n",
    "\n",
    " - Once we run all cells of the notebook, we have essentially pushed a pandas dataframe of predictions to Google BigQuery. The dataframe itself looks like the following:\n",
    "\n",
    "\n",
    "![nb_output](images/nb_output.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - we should be able to see these predictions on the Google cloud console. So lets open up the Google Console homepage.\n",
    "\n",
    "![bq1](images/bq1.png)\n",
    "\n",
    "\n",
    " - From the console homepage, navigating to BigQuery lands us the following page. \n",
    "\n",
    "![bq2](images/bq2.png)\n",
    "\n",
    "\n",
    " - We are not interested in the SQL editor at the moment. At the bottom right, we can see our project.\n",
    "\n",
    "![bq3](images/bq3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Expand the project on the left column to get to the `movie_recommendation_service` database and then to the `predicted_movies` table. The default information is the schema.\n",
    "\n",
    "![bq4](images/bq4.png)\n",
    "\n",
    " - Changing from the scema tab to the preview tab shows that the upload was successful.\n",
    "\n",
    "![bq5](images/bq5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Lets rerun the notebook from the commandline. I am assuming that the `model-user.json` is in the current directory for simplicity. This way, we don't have to set the environment variable `GOOGLE_APPLICATION_CREDENTIALS`.\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:pipelines theja$ jupyter nbconvert --to notebook --ExecutePreprocessor.kernel_name=python3 --inplace --execute recommend_lightfm.ipynb\n",
    "[NbConvertApp] Converting notebook recommend_lightfm.ipynb to notebook\n",
    "[NbConvertApp] Executing notebook with kernel: python3\n",
    "[NbConvertApp] Writing 11592 bytes to recommend_lightfm.ipynb\n",
    "```\n",
    " - Going back to the BigQuery interface, the only thing that has changed is the timestamp when the predictions were generated (previewed results may not retrieve the same user-ids).\n",
    "\n",
    " ![bq6](images/bq6.png)\n",
    "\n",
    " - Querying from this table can also be done from a notebook:\n",
    "   - [Download notebook locally](recommend_query_bigquery.ipynb)\n",
    "\n",
    "![query_bq](images/query_bq.png)\n",
    "\n",
    " - You will notice that the format of the returned recommendations is not easy to parse. So a good exercise challenge is to use regular expressions on the output or modify the way predictions are generated (see Exercises).\n",
    "\n",
    "![bad_format](images/bad_format.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Remark\n",
    "\n",
    " - While we did all four blocks in a single script, it makes sense to break it into 4 blocks (fetch data, train, predict, send predictions to database service). This, way a block can retry its execution if the previous block fails and is manually handled by a team member. In particular, this retry can be automated using scheduling tools. One such tool is `cron`, which is the subject of the next section. \n",
    "\n",
    " - For simplicity we will containerize the four blocks of this transient model pipeline into a single container and script (i.e., retain the above). Our next tool will allow us to run it automatically in a periodic manner.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
