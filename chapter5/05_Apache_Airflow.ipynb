{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    " - While cron and cron based scheduling is great, it becomes harder to manage if certain jobs fail and other scheduled jobs depend on their outputs.\n",
    "\n",
    " - Workflow tools help with resolving these types of dependencies. \n",
    "\n",
    " - They also allow for version control of objects beyond code.\n",
    "\n",
    " - These tools have additional capabilities such as alerting team members if a block/task/job failed so that someone can fix and even manually run it.\n",
    "\n",
    " - It is beneficial for the whole organization if they can use a similar tool:\n",
    " \t- data engineers doing ETL jobs, \n",
    " \t- data scientists doing model trainng jobs,\n",
    " \t- analysts doing reporting jobs etc.\n",
    "\n",
    " - We will go through [Apache Airflow](https://airflow.apache.org/) as an example workflow tool. There are many others, as we mentioned before.\n",
    "\n",
    "![airflow1](images/airflow1.png)\n",
    "\t<div style=\"text-align: right\"> Source: https://airflow.apache.org/ </div>\n",
    "\n",
    " - As listed above, a key benefit with airflow is that it allows us to describe a ML pipeline in code (and in python!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Airflow Basics\n",
    "\n",
    " - Airflow works with graphs (spcifically, directed acyclic graphs or DAGs) that relate tasks to each other and describe their ordering.\n",
    "\n",
    " - Each node in the DAG is a task, with incoming arrows from other tasks implying that they are *upstream* dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Lets install the airflow package and get a server running. From the [quickstart page](https://airflow.apache.org/docs/stable/start.html)\n",
    "\n",
    "```bash\n",
    "# airflow needs a home, ~/airflow is the default,\n",
    "# but you can lay foundation somewhere else if you prefer\n",
    "# (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# install from pypi using pip\n",
    "pip install apache-airflow\n",
    "\n",
    "# initialize the database\n",
    "airflow initdb\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver -p 8080\n",
    "\n",
    "# start the scheduler\n",
    "airflow scheduler\n",
    "\n",
    "# visit localhost:8080 in the browser and enable the example dag in the home page\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - For instance, when you start the webserver, you should seen an output similar to below:\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:lec05 theja$ airflow webserver -p 8080\n",
    "  ____________       _____________\n",
    " ____    |__( )_________  __/__  /________      __\n",
    "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
    "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
    " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
    "[2020-09-24 12:55:50,012] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
    "[2020-09-24 12:55:50,012] {dagbag.py:417} INFO - Filling up the DagBag from /Users/theja/airflow/dags\n",
    "/Users/theja/miniconda3/envs/datasci-dev/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
    "  category=PendingDeprecationWarning)\n",
    "Running the Gunicorn Server with:\n",
    "Workers: 4 sync\n",
    "Host: 0.0.0.0:8080\n",
    "Timeout: 120\n",
    "Logfiles: - -\n",
    ".\n",
    ".\n",
    "(truncated)\n",
    ".\n",
    ".\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Similarly when the scheduler is started, you should see:\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:lec05 theja$ airflow scheduler\n",
    "  ____________       _____________\n",
    " ____    |__( )_________  __/__  /________      __\n",
    "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
    "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
    " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
    "[2020-09-24 12:57:27,736] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
    "[2020-09-24 12:57:27,774] {scheduler_job.py:1367} INFO - Starting the scheduler\n",
    "[2020-09-24 12:57:27,775] {scheduler_job.py:1375} INFO - Running execute loop for -1 seconds\n",
    "[2020-09-24 12:57:27,775] {scheduler_job.py:1376} INFO - Processing each file at most -1 times\n",
    "[2020-09-24 12:57:27,775] {scheduler_job.py:1379} INFO - Searching for files in /Users/theja/airflow/dags\n",
    "[2020-09-24 12:57:27,785] {scheduler_job.py:1381} INFO - There are 25 files in /Users/theja/airflow/dags\n",
    "[2020-09-24 12:57:27,785] {scheduler_job.py:1438} INFO - Resetting orphaned tasks for active dag runs\n",
    "[2020-09-24 12:57:27,802] {dag_processing.py:562} INFO - Launched DagFileProcessorManager with pid: 5109\n",
    "[2020-09-24 12:57:27,812] {settings.py:55} INFO - Configured default timezone <Timezone [UTC]>\n",
    "[2020-09-24 12:57:27,829] {dag_processing.py:776} WARNING - Because we cannot use more than 1 thread (max_threads = 2) when using sqlite. So we set parallelism to 1.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Following this, we can go to `localhost:8080` to see the follwoing:\n",
    "\n",
    "![airflowweb1](images/airflowweb1.png)\n",
    "\n",
    " - When the above sequence of commands was ran, airflow created a config file in `~/airflow` folder. This config file has about 1000 lines.\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:~ theja$ cd airflow/\n",
    "(datasci-dev) ttmac:airflow theja$ less airflow.cfg\n",
    "[core]\n",
    "# The folder where your airflow pipelines live, most likely a\n",
    "# subfolder in a code repository. This path must be absolute.\n",
    "dags_folder = /Users/theja/airflow/dags\n",
    "\n",
    "# The folder where airflow should store its log files\n",
    "# This path must be absolute\n",
    "base_log_folder = /Users/theja/airflow/logs\n",
    ".\n",
    ".\n",
    "(truncated)\n",
    ".\n",
    ".\n",
    "(datasci-dev) ttmac:airflow theja$ wc -l airflow.cfg\n",
    "    1073 airflow.cfg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Airflow manages information about pipelines through a database. By default is it `sqlite` (we could change this to something [else](https://airflow.apache.org/docs/stable/howto/initialize-database.html) if needed). This is initialized via the initdb argument.\n",
    "\n",
    " - The scheduler executes out tasks on workers (machines).\n",
    "\n",
    " - The webserver allows us to interact with the task scheduler and the database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Anatomy of a Workflow Specification\n",
    "\n",
    " - The key idea is that We need to create a python file to define the workflow DAG.\n",
    " \n",
    " - A key module that we will import is called the BashOperator, which allows us to run arbitrary commands (e.g., `docker run image`) as long as the dependencies are there (e.g., the `docker` daemon, the local image registry, and command line utility).\n",
    " - There are a set of parameters that one should set for any workflow. For instance, who is the owner of this workflow, and if they need to be alerted by email.\n",
    "\n",
    " - We next create an instance of the DAG class.\n",
    "\n",
    " - We can define our command line task using the BashOperator. There are various kinds of operators available. We will also make it a node in our DAG.\n",
    "\n",
    " - If there are additional tasks, we related them to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### A very quick start using the tutorial workflow\n",
    "\n",
    " - Lets start by going through the [tutorial](https://airflow.apache.org/docs/stable/tutorial.html) in their documentation. After that we will run our transient pipeline as a workflow through airflow. Below is a gist/anatomy of a workflow specification.\n",
    "\n",
    " - We can execute the tutorial workflow by using the following command:\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:dags theja$ airflow backfill tutorial -s 2020-09-20 -e 2020-09-22\n",
    "```\n",
    "\n",
    " - We can watch the progress in the browser by going to Browse -> Task Instances. You can see the progress snapshots below.\n",
    "\n",
    "![tut1](images/tut1.png)\n",
    "\n",
    "![tut2](images/tut2.png)\n",
    "\n",
    " - Here it shows the successful completion of all tasks.\n",
    "\n",
    "![tut3](images/tut3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Workflow Specification for Transient Training Pipeline\n",
    "\n",
    " - We can specify our workflow in the `~/airflow/dags` folder as `recommend.py`, which will get picked up automatically by the scheduler. \n",
    "  - [Download locally](recommend_airflow.py)\n",
    "\n",
    " - If it is not automatically added, try running the following command:\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:dags theja$ airflow list_dags\n",
    "[2020-09-24 15:02:44,915] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
    "[2020-09-24 15:02:44,915] {dagbag.py:417} INFO - Filling up the DagBag from /Users/theja/airflow/dags\n",
    "/Users/theja/miniconda3/envs/datasci-dev/lib/python3.7/site-packages/airflow/models/dag.py:1342: PendingDeprecationWarning: The requested task could not be added to the DAG because a task with task_id create_tag_template_field_result is already in the DAG. Starting in Airflow 2.0, trying to overwrite a task will raise an exception.\n",
    "  category=PendingDeprecationWarning)\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "DAGS\n",
    "-------------------------------------------------------------------\n",
    "example_bash_operator\n",
    "example_branch_dop_operator_v3\n",
    "example_branch_operator\n",
    "example_complex\n",
    "example_external_task_marker_child\n",
    "example_external_task_marker_parent\n",
    "example_http_operator\n",
    "example_kubernetes_executor_config\n",
    "example_nested_branch_dag\n",
    "example_passing_params_via_test_command\n",
    "example_pig_operator\n",
    "example_python_operator\n",
    "example_short_circuit_operator\n",
    "example_skip_dag\n",
    "example_subdag_operator\n",
    "example_subdag_operator.section-1\n",
    "example_subdag_operator.section-2\n",
    "example_trigger_controller_dag\n",
    "example_trigger_target_dag\n",
    "example_xcom\n",
    "latest_only\n",
    "latest_only_with_trigger\n",
    "recommend-pipeline\n",
    "test_utils\n",
    "tutorial\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - Our python script's contents are reproduced below (to check for syntax issues just run the py file on the commandline):\n",
    "\n",
    "```python\n",
    "# [START import_module]\n",
    "from datetime import timedelta\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to operate!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "# [END import_module]\n",
    "\n",
    "# [START default_args]\n",
    "# These args will get passed on to each operator\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': days_ago(2),\n",
    "    'email': ['myself@theja.org'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'sla': timedelta(hours=2),\n",
    "    # 'execution_timeout': timedelta(seconds=300),\n",
    "    # 'on_failure_callback': some_function,\n",
    "    # 'on_success_callback': some_other_function,\n",
    "    # 'on_retry_callback': another_function,\n",
    "    # 'sla_miss_callback': yet_another_function,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}\n",
    "# [END default_args]\n",
    "\n",
    "# [START instantiate_dag]\n",
    "dag = DAG(\n",
    "    'recommend-pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Run the transient training pipeline',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "# [END instantiate_dag]\n",
    "\n",
    "t1 = BashOperator(\n",
    "    task_id='docker-pipeline-run',\n",
    "    bash_command='docker run recommend_pipeline',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "# [START documentation]\n",
    "dag.doc_md = __doc__\n",
    "\n",
    "t1.doc_md = \"\"\"\\\n",
    "#### Transient Pipeline\n",
    "Downloads movielens-100k, trains a recommendation model and saves top 10 recommendations to Google BigQuery.\n",
    "\"\"\"\n",
    "# [END documentation]\n",
    "\n",
    "\n",
    "t1\n",
    "# [END tutorial]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - The task can be seen from the browser UI as well:\n",
    "\n",
    "![rec1](images/rec1.png)\n",
    "\n",
    "\n",
    " - We can run this workflow by triggering it through the UI or by using the backfill argument.\n",
    "\n",
    "```bash\n",
    "(datasci-dev) ttmac:dags theja$ airflow backfill recommend-pipeline -s 2020-09-01 -e 2020-09-01\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - We can verify that the task ran successfully in the browser.\n",
    "\n",
    "![rec2](images/rec2.png)\n",
    "\n",
    " - We can also check that the timestamp of update in BigQuery reflects the successful completion of the transient training pipeline.\n",
    "\n",
    "![rec2](images/rec2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Remarks\n",
    "\n",
    " - If there were other tasks, they can be specified similarly and can be related to each other in the script using `.set_upstream()` function (there are other ways, we already saw one in the tutorial).\n",
    "\n",
    " - Instead of the BashOperator, we can also use DockerOperator (we haven't done this here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
